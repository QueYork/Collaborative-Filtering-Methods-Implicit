{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0900197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setting\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('/content/drive/MyDrive/SDSC3002_Project/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce33bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c666ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPR, self).__init__()\n",
    "        self.W = None             # user matrix\n",
    "        self.H = None             # item matrix\n",
    "        \n",
    "        self.uid = None\n",
    "        self.iid = None\n",
    "        \n",
    "        # 用户u对应他访问过的所有items集合\n",
    "        self.train_user_items = None\n",
    "        self.test_user_items = None\n",
    "        \n",
    "        # (u, i, rating) dataset\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "        \n",
    "    def _split(self, df, ratio):\n",
    "        train = pd.DataFrame(columns = df.columns, dtype=int)\n",
    "        test = pd.DataFrame(columns = df.columns, dtype=int)\n",
    "        for i in self.uid:\n",
    "            train_1, test_1 = train_test_split(df[df.iloc[:, 0] == i], train_size = ratio, shuffle = True, random_state = 5)\n",
    "            train = pd.concat([train, train_1])\n",
    "            test = pd.concat([test, test_1])\n",
    "        return train, test    \n",
    "    \n",
    "    def preprocess(self, df, train_size=0.8):\n",
    "        df = df.rename(columns = {df.columns[0]: 'ori_uid', df.columns[1]: 'ori_iid', df.columns[2]: 'rating'})\n",
    "        df = df.groupby('ori_uid').filter(lambda x: x['ori_uid'].count()>=10)\n",
    "        uid_map = pd.DataFrame({\"ori_uid\": np.asarray(list(set(df.iloc[:,0].values)))})\n",
    "        uid_map[\"serial_uid\"] = uid_map.index\n",
    "        iid_map = pd.DataFrame({\"ori_iid\": np.asarray(list(set(df.iloc[:,1].values)))})\n",
    "        iid_map[\"serial_iid\"] = iid_map.index\n",
    "        \n",
    "        self.uid = uid_map[\"serial_uid\"].values\n",
    "        self.iid = iid_map[\"serial_iid\"].values\n",
    "        \n",
    "        df = df.merge(uid_map, left_on = 'ori_uid', right_on = 'ori_uid', how=\"left\")\n",
    "        df = df.merge(iid_map, left_on = 'ori_iid', right_on = 'ori_iid', how=\"left\")\n",
    "        df = df[['serial_uid', 'serial_iid', 'rating']]\n",
    "        \n",
    "        train, test = self._split(df, train_size)\n",
    "        \n",
    "        self.train_user_items = train.groupby(train.columns[0])[train.columns[1]].apply(lambda x: list(x)).to_list()\n",
    "        self.test_user_items = test.groupby(test.columns[0])[test.columns[1]].apply(lambda x: list(x)).to_list()\n",
    "        \n",
    "        self.train = train\n",
    "        self.test = test\n",
    "    \n",
    "    def generate_train_batch(self, batch, sets):\n",
    "        train = []\n",
    "        for b in range(batch):\n",
    "            u = self.uid[np.random.randint(0, len(self.uid))]\n",
    "            i = sets[u][np.random.randint(0, len(sets[u]))]\n",
    "            j = self.iid[np.random.randint(0, len(self.iid))]\n",
    "            while j in sets[u]:\n",
    "                j = self.iid[np.random.randint(0, len(self.iid))]\n",
    "            train.append([u, i, j])\n",
    "        return np.asarray(train) \n",
    "    \n",
    "    def fit(self, k, stepsize=0.05, max_iter=10, batch=10000, n=10):\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        \n",
    "        self.W = nn.Parameter(torch.rand(len(self.uid), k).to(device) * 0.01)    # 初始化 W，H\n",
    "        self.H = nn.Parameter(torch.rand(len(self.iid), k).to(device) * 0.01)  \n",
    "        \n",
    "        # 创建字典：用户u对应他访问过的所有items集合\n",
    "#         self.train_user_items = df.groupby(df.columns[0])[df.columns[1]].apply(lambda x: np.array(x)).to_list()\n",
    "        \n",
    "        optimizer = optim.Adam([self.W, self.H], lr=stepsize)     # 主模型优化器\n",
    "        for x in range(max_iter):\n",
    "            #取训练批次：uij三元组\n",
    "            uij = self.generate_train_batch(batch, self.train_user_items)\n",
    "            \n",
    "            u = uij[:, 0]\n",
    "            i = uij[:, 1]\n",
    "            j = uij[:, 2]\n",
    "            u_emb = self.W[u]\n",
    "            i_emb = self.H[i]\n",
    "            j_emb = self.H[j]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = -torch.sum(torch.log(torch.sigmoid(torch.sum(u_emb * (i_emb - j_emb),dim = 1))))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f\"Train | {x+1}/{max_iter}, BPR loss: {loss.item() / batch}\")\n",
    "            \n",
    "            rec, pre, ndcg = self.performance(n)\n",
    "            print(f'Valid | {x+1}/{max_iter}, Pre@{n}: {pre}, Rec@{n}: {rec}, NDCG@{n}: {ndcg}')\n",
    "            print('---------------------------------------------------')\n",
    "    \n",
    "    def _predict(self, uid, items, n):\n",
    "        scores = torch.mv(self.H[items], self.W[uid])\n",
    "        if n > scores.shape[0]: \n",
    "            n = scores.shape[0]\n",
    "        top_N_val, top_N_idx = torch.topk(scores, k=n)\n",
    "        return list(zip(items[top_N_idx.cpu()], top_N_val.cpu()))\n",
    "\n",
    "    def NDCG(self, uid, n):         # 用模型排序+真实分数计算 DCG, 重排后计算 iDCG\n",
    "        # test 集中，uid 评过的 items\n",
    "        test_user = self.test[self.test.iloc[:, 0] == uid]\n",
    "        \n",
    "        # 对这些 items 做 top-k\n",
    "        rating = self._predict(uid, test_user.iloc[:, 1].values, n)\n",
    "        \n",
    "        # 排序真实评分\n",
    "        irating =sorted(test_user.iloc[:, 2].values, reverse=True)\n",
    "        irating = np.asarray(irating)\n",
    "        \n",
    "        if n > len(irating): n = len(irating)  \n",
    "            \n",
    "        # 取出模型排序下 merge 到的真实分数    \n",
    "        rating_df = pd.DataFrame(rating, columns=['serial_iid', 'pred_rating'])\n",
    "        merged_df = pd.merge(rating_df, test_user, on='serial_iid')\n",
    "        r = np.array(merged_df['rating'])\n",
    "        \n",
    "        # 求 log 分母\n",
    "        log = np.log(np.arange(2, n + 2)) \n",
    "        \n",
    "        # 求 dcg 和 idcg\n",
    "        dcg = np.log(2) * np.sum((2**r[:n] - 1) / log)\n",
    "        idcg = np.log(2) * np.sum((2**irating[:n] - 1) / log)\n",
    "        \n",
    "        return dcg / idcg\n",
    "\n",
    "    def performance(self, n):      # Output recall@n, precision@n, NDCG@n\n",
    "        hit = 0\n",
    "        n_recall = 0\n",
    "        n_precision = 0\n",
    "        ndcg = 0\n",
    "        \n",
    "        for i in self.uid:\n",
    "            # Items that User i hasn't tried in training set\n",
    "            unknown_items = np.setdiff1d(self.iid, self.train_user_items[i])\n",
    "            # Items that User i actually tried in testing set\n",
    "            known_items = self.test_user_items[i]\n",
    "            \n",
    "            #目标：预测 unknown items 中的top_N，若击中test中的items，则为有效预测\n",
    "            ru = self._predict(i, unknown_items, n)\n",
    "            \n",
    "            hit += sum(1 for item, pui in ru if item in known_items)\n",
    "            n_recall += len(known_items)\n",
    "            n_precision += n\n",
    "            ndcg += self.NDCG(i, n)\n",
    "            \n",
    "        recall = hit / (1.0 * n_recall)\n",
    "        precision = hit / (1.0 * n_precision)\n",
    "        ndcg /= len(self.uid)\n",
    "        return recall, precision, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319ffcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./ml-100k/u.data\", sep=\"\\t\", names=['user id', 'item id', 'rating', 'timestamp'])\n",
    "df2 = pd.read_csv(\"./ml-1m/ratings.dat\", sep=\"::\", names=['user id', 'item id', 'rating', 'timestamp'], engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2cdfb1",
   "metadata": {},
   "source": [
    "### 100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da1781ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1 = BPR()\n",
    "model1.preprocess(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26dbdb06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 1/100, BPR loss: 0.69314052734375\n",
      "Valid | 1/100, Pre@10: 0.018663838812301166, Recl@10: 0.008635493842304106, NDCG@10: 0.6945220327132166\n",
      "---------------------------------------------------\n",
      "Train | 2/100, BPR loss: 0.6847896484375\n",
      "Valid | 2/100, Pre@10: 0.11717921527041357, Recl@10: 0.05421716304401158, NDCG@10: 0.7314359293011433\n",
      "---------------------------------------------------\n",
      "Train | 3/100, BPR loss: 0.614569580078125\n",
      "Valid | 3/100, Pre@10: 0.13488865323435842, Recl@10: 0.06241106913301604, NDCG@10: 0.7322834425931577\n",
      "---------------------------------------------------\n",
      "Train | 4/100, BPR loss: 0.50909267578125\n",
      "Valid | 4/100, Pre@10: 0.1354188759278897, Recl@10: 0.06265639566262696, NDCG@10: 0.7335292770190222\n",
      "---------------------------------------------------\n",
      "Train | 5/100, BPR loss: 0.400266357421875\n",
      "Valid | 5/100, Pre@10: 0.1395546129374337, Recl@10: 0.06456994259359207, NDCG@10: 0.7364553319342508\n",
      "---------------------------------------------------\n",
      "Train | 6/100, BPR loss: 0.3485695068359375\n",
      "Valid | 6/100, Pre@10: 0.14750795334040298, Recl@10: 0.06824984053775575, NDCG@10: 0.7399917888003709\n",
      "---------------------------------------------------\n",
      "Train | 7/100, BPR loss: 0.3210808837890625\n",
      "Valid | 7/100, Pre@10: 0.1537645811240721, Recl@10: 0.07114469358716452, NDCG@10: 0.7412852815889691\n",
      "---------------------------------------------------\n",
      "Train | 8/100, BPR loss: 0.3311087646484375\n",
      "Valid | 8/100, Pre@10: 0.1586426299045599, Recl@10: 0.0734016976595849, NDCG@10: 0.7396051321370235\n",
      "---------------------------------------------------\n",
      "Train | 9/100, BPR loss: 0.3189212158203125\n",
      "Valid | 9/100, Pre@10: 0.16076352067868505, Recl@10: 0.07438300377802856, NDCG@10: 0.7387048706906109\n",
      "---------------------------------------------------\n",
      "Train | 10/100, BPR loss: 0.3370717529296875\n",
      "Valid | 10/100, Pre@10: 0.16076352067868505, Recl@10: 0.07438300377802856, NDCG@10: 0.7373990906351128\n",
      "---------------------------------------------------\n",
      "Train | 11/100, BPR loss: 0.332572998046875\n",
      "Valid | 11/100, Pre@10: 0.16288441145281018, Recl@10: 0.0753643098964722, NDCG@10: 0.7371037115550799\n",
      "---------------------------------------------------\n",
      "Train | 12/100, BPR loss: 0.341414404296875\n",
      "Valid | 12/100, Pre@10: 0.16606574761399787, Recl@10: 0.07683626907413768, NDCG@10: 0.7363918690201438\n",
      "---------------------------------------------------\n",
      "Train | 13/100, BPR loss: 0.350870947265625\n",
      "Valid | 13/100, Pre@10: 0.16489925768822905, Recl@10: 0.07629655070899367, NDCG@10: 0.7367028924609283\n",
      "---------------------------------------------------\n",
      "Train | 14/100, BPR loss: 0.3366233642578125\n",
      "Valid | 14/100, Pre@10: 0.16436903499469777, Recl@10: 0.07605122417938276, NDCG@10: 0.7357474770915609\n",
      "---------------------------------------------------\n",
      "Train | 15/100, BPR loss: 0.309087451171875\n",
      "Valid | 15/100, Pre@10: 0.1657476139978791, Recl@10: 0.07668907315637113, NDCG@10: 0.7345739935312016\n",
      "---------------------------------------------------\n",
      "Train | 16/100, BPR loss: 0.31960068359375\n",
      "Valid | 16/100, Pre@10: 0.1672322375397667, Recl@10: 0.07737598743928169, NDCG@10: 0.7334112548857462\n",
      "---------------------------------------------------\n",
      "Train | 17/100, BPR loss: 0.3149239013671875\n",
      "Valid | 17/100, Pre@10: 0.16691410392364794, Recl@10: 0.07722879152151514, NDCG@10: 0.7329752460042255\n",
      "---------------------------------------------------\n",
      "Train | 18/100, BPR loss: 0.30041669921875\n",
      "Valid | 18/100, Pre@10: 0.16564156945917285, Recl@10: 0.07664000785044894, NDCG@10: 0.7345908287662227\n",
      "---------------------------------------------------\n",
      "Train | 19/100, BPR loss: 0.289753857421875\n",
      "Valid | 19/100, Pre@10: 0.16542948038176034, Recl@10: 0.07654187723860459, NDCG@10: 0.7352776439396137\n",
      "---------------------------------------------------\n",
      "Train | 20/100, BPR loss: 0.275786767578125\n",
      "Valid | 20/100, Pre@10: 0.16765641569459172, Recl@10: 0.07757224866297041, NDCG@10: 0.7354580808730684\n",
      "---------------------------------------------------\n",
      "Train | 21/100, BPR loss: 0.2811582763671875\n",
      "Valid | 21/100, Pre@10: 0.16977730646871686, Recl@10: 0.07855355478141406, NDCG@10: 0.7379194068217382\n",
      "---------------------------------------------------\n",
      "Train | 22/100, BPR loss: 0.2695627197265625\n",
      "Valid | 22/100, Pre@10: 0.17582184517497348, Recl@10: 0.08135027721897846, NDCG@10: 0.7394354140816828\n",
      "---------------------------------------------------\n",
      "Train | 23/100, BPR loss: 0.2586943359375\n",
      "Valid | 23/100, Pre@10: 0.17889713679745492, Recl@10: 0.08277317109072176, NDCG@10: 0.7390957985164324\n",
      "---------------------------------------------------\n",
      "Train | 24/100, BPR loss: 0.246174755859375\n",
      "Valid | 24/100, Pre@10: 0.1844114528101803, Recl@10: 0.08532456699867523, NDCG@10: 0.7399022541555345\n",
      "---------------------------------------------------\n",
      "Train | 25/100, BPR loss: 0.24013330078125\n",
      "Valid | 25/100, Pre@10: 0.1909862142099682, Recl@10: 0.08836661596585055, NDCG@10: 0.7415275241312546\n",
      "---------------------------------------------------\n",
      "Train | 26/100, BPR loss: 0.2318243896484375\n",
      "Valid | 26/100, Pre@10: 0.20116648992576883, Recl@10: 0.09307688533438006, NDCG@10: 0.7411385597839294\n",
      "---------------------------------------------------\n",
      "Train | 27/100, BPR loss: 0.224371923828125\n",
      "Valid | 27/100, Pre@10: 0.20827147401908802, Recl@10: 0.09636426083116628, NDCG@10: 0.7410128688550351\n",
      "---------------------------------------------------\n",
      "Train | 28/100, BPR loss: 0.20695263671875\n",
      "Valid | 28/100, Pre@10: 0.21792152704135737, Recl@10: 0.10082920367008488, NDCG@10: 0.7405651306663722\n",
      "---------------------------------------------------\n",
      "Train | 29/100, BPR loss: 0.20721201171875\n",
      "Valid | 29/100, Pre@10: 0.2241781548250265, Recl@10: 0.10372405671949364, NDCG@10: 0.7412015363271497\n",
      "---------------------------------------------------\n",
      "Train | 30/100, BPR loss: 0.1890358642578125\n",
      "Valid | 30/100, Pre@10: 0.22608695652173913, Recl@10: 0.10460723222609293, NDCG@10: 0.7402624564840286\n",
      "---------------------------------------------------\n",
      "Train | 31/100, BPR loss: 0.18596458740234376\n",
      "Valid | 31/100, Pre@10: 0.23244962884411452, Recl@10: 0.10755115058142388, NDCG@10: 0.7394812098526949\n",
      "---------------------------------------------------\n",
      "Train | 32/100, BPR loss: 0.17769732666015625\n",
      "Valid | 32/100, Pre@10: 0.23679745493107104, Recl@10: 0.10956282812423336, NDCG@10: 0.7378709330865222\n",
      "---------------------------------------------------\n",
      "Train | 33/100, BPR loss: 0.17557427978515625\n",
      "Valid | 33/100, Pre@10: 0.2408271474019088, Recl@10: 0.11142730974927628, NDCG@10: 0.7372036438153017\n",
      "---------------------------------------------------\n",
      "Train | 34/100, BPR loss: 0.16765887451171874\n",
      "Valid | 34/100, Pre@10: 0.24782608695652175, Recl@10: 0.11466561994014032, NDCG@10: 0.7364452658681647\n",
      "---------------------------------------------------\n",
      "Train | 35/100, BPR loss: 0.16506175537109374\n",
      "Valid | 35/100, Pre@10: 0.24994697773064686, Recl@10: 0.11564692605858398, NDCG@10: 0.7377563030652838\n",
      "---------------------------------------------------\n",
      "Train | 36/100, BPR loss: 0.1580077392578125\n",
      "Valid | 36/100, Pre@10: 0.251643690349947, Recl@10: 0.11643197095333889, NDCG@10: 0.7387730044592284\n",
      "---------------------------------------------------\n",
      "Train | 37/100, BPR loss: 0.1551242919921875\n",
      "Valid | 37/100, Pre@10: 0.2550371155885472, Recl@10: 0.11800206074284873, NDCG@10: 0.7387353756245284\n",
      "---------------------------------------------------\n",
      "Train | 38/100, BPR loss: 0.1427762939453125\n",
      "Valid | 38/100, Pre@10: 0.25790031813361614, Recl@10: 0.11932682400274766, NDCG@10: 0.7393657412281378\n",
      "---------------------------------------------------\n",
      "Train | 39/100, BPR loss: 0.144425732421875\n",
      "Valid | 39/100, Pre@10: 0.26468716861081654, Recl@10: 0.12246700358176733, NDCG@10: 0.7394944201859648\n",
      "---------------------------------------------------\n",
      "Train | 40/100, BPR loss: 0.14256221923828125\n",
      "Valid | 40/100, Pre@10: 0.2657476139978791, Recl@10: 0.12295765664098916, NDCG@10: 0.7403729297182337\n",
      "---------------------------------------------------\n",
      "Train | 41/100, BPR loss: 0.1377477783203125\n",
      "Valid | 41/100, Pre@10: 0.26882290562036054, Recl@10: 0.12438055051273245, NDCG@10: 0.7397964259973105\n",
      "---------------------------------------------------\n",
      "Train | 42/100, BPR loss: 0.13305303955078124\n",
      "Valid | 42/100, Pre@10: 0.26786850477200425, Recl@10: 0.12393896275943281, NDCG@10: 0.7408043232293671\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 43/100, BPR loss: 0.13249813232421875\n",
      "Valid | 43/100, Pre@10: 0.26871686108165427, Recl@10: 0.12433148520681027, NDCG@10: 0.7386429335652778\n",
      "---------------------------------------------------\n",
      "Train | 44/100, BPR loss: 0.12674990234375\n",
      "Valid | 44/100, Pre@10: 0.2715800636267232, Recl@10: 0.1256562484667092, NDCG@10: 0.7396281238602823\n",
      "---------------------------------------------------\n",
      "Train | 45/100, BPR loss: 0.12901617431640625\n",
      "Valid | 45/100, Pre@10: 0.2708377518557794, Recl@10: 0.1253127913252539, NDCG@10: 0.7397452604619902\n",
      "---------------------------------------------------\n",
      "Train | 46/100, BPR loss: 0.1233433349609375\n",
      "Valid | 46/100, Pre@10: 0.2704135737009544, Recl@10: 0.12511653010156518, NDCG@10: 0.7376478393393293\n",
      "---------------------------------------------------\n",
      "Train | 47/100, BPR loss: 0.11816593017578125\n",
      "Valid | 47/100, Pre@10: 0.27104984093319195, Recl@10: 0.12541092193709827, NDCG@10: 0.7378320018403769\n",
      "---------------------------------------------------\n",
      "Train | 48/100, BPR loss: 0.1268883544921875\n",
      "Valid | 48/100, Pre@10: 0.2725344644750795, Recl@10: 0.12609783622000884, NDCG@10: 0.7386759500507944\n",
      "---------------------------------------------------\n",
      "Train | 49/100, BPR loss: 0.12423035888671875\n",
      "Valid | 49/100, Pre@10: 0.27274655355249205, Recl@10: 0.1261959668318532, NDCG@10: 0.7402459049260925\n",
      "---------------------------------------------------\n",
      "Train | 50/100, BPR loss: 0.1178418701171875\n",
      "Valid | 50/100, Pre@10: 0.2713679745493107, Recl@10: 0.12555811785486481, NDCG@10: 0.7408142147015097\n",
      "---------------------------------------------------\n",
      "Train | 51/100, BPR loss: 0.118999755859375\n",
      "Valid | 51/100, Pre@10: 0.2706256627783669, Recl@10: 0.12521466071340956, NDCG@10: 0.7407726064365328\n",
      "---------------------------------------------------\n",
      "Train | 52/100, BPR loss: 0.1228186767578125\n",
      "Valid | 52/100, Pre@10: 0.27285259809119833, Recl@10: 0.12624503213777538, NDCG@10: 0.7419803976659421\n",
      "---------------------------------------------------\n",
      "Train | 53/100, BPR loss: 0.10887354736328125\n",
      "Valid | 53/100, Pre@10: 0.271898197242842, Recl@10: 0.12580344438447574, NDCG@10: 0.7418095937898018\n",
      "---------------------------------------------------\n",
      "Train | 54/100, BPR loss: 0.1112472900390625\n",
      "Valid | 54/100, Pre@10: 0.2699893955461294, Recl@10: 0.12492026887787645, NDCG@10: 0.7414938880090928\n",
      "---------------------------------------------------\n",
      "Train | 55/100, BPR loss: 0.115285498046875\n",
      "Valid | 55/100, Pre@10: 0.2694591728525981, Recl@10: 0.12467494234826554, NDCG@10: 0.7433570523452785\n",
      "---------------------------------------------------\n",
      "Train | 56/100, BPR loss: 0.11324920654296874\n",
      "Valid | 56/100, Pre@10: 0.271474019088017, Recl@10: 0.125607183160787, NDCG@10: 0.7436294166988372\n",
      "---------------------------------------------------\n",
      "Train | 57/100, BPR loss: 0.103908203125\n",
      "Valid | 57/100, Pre@10: 0.2735949098621421, Recl@10: 0.12658848927923066, NDCG@10: 0.7433239913368928\n",
      "---------------------------------------------------\n",
      "Train | 58/100, BPR loss: 0.1119279052734375\n",
      "Valid | 58/100, Pre@10: 0.27179215270413576, Recl@10: 0.12575437907855355, NDCG@10: 0.7429437441423384\n",
      "---------------------------------------------------\n",
      "Train | 59/100, BPR loss: 0.102594970703125\n",
      "Valid | 59/100, Pre@10: 0.2711558854718982, Recl@10: 0.12545998724302046, NDCG@10: 0.7437924576414588\n",
      "---------------------------------------------------\n",
      "Train | 60/100, BPR loss: 0.104085595703125\n",
      "Valid | 60/100, Pre@10: 0.271898197242842, Recl@10: 0.12580344438447574, NDCG@10: 0.744808848321863\n",
      "---------------------------------------------------\n",
      "Train | 61/100, BPR loss: 0.1189428955078125\n",
      "Valid | 61/100, Pre@10: 0.2698833510074231, Recl@10: 0.12487120357195428, NDCG@10: 0.7438857453574087\n",
      "---------------------------------------------------\n",
      "Train | 62/100, BPR loss: 0.1002064697265625\n",
      "Valid | 62/100, Pre@10: 0.2707317073170732, Recl@10: 0.12526372601933172, NDCG@10: 0.744397249850136\n",
      "---------------------------------------------------\n",
      "Train | 63/100, BPR loss: 0.10364537353515625\n",
      "Valid | 63/100, Pre@10: 0.2692470837751856, Recl@10: 0.12457681173642117, NDCG@10: 0.7430860551668963\n",
      "---------------------------------------------------\n",
      "Train | 64/100, BPR loss: 0.105785498046875\n",
      "Valid | 64/100, Pre@10: 0.268186638388123, Recl@10: 0.12408615867719935, NDCG@10: 0.7417974898606642\n",
      "---------------------------------------------------\n",
      "Train | 65/100, BPR loss: 0.10954327392578125\n",
      "Valid | 65/100, Pre@10: 0.2679745493107105, Recl@10: 0.12398802806535499, NDCG@10: 0.7406943476651341\n",
      "---------------------------------------------------\n",
      "Train | 66/100, BPR loss: 0.1005390625\n",
      "Valid | 66/100, Pre@10: 0.26808059384941674, Recl@10: 0.12403709337127718, NDCG@10: 0.7384936677358873\n",
      "---------------------------------------------------\n",
      "Train | 67/100, BPR loss: 0.09585621337890625\n",
      "Valid | 67/100, Pre@10: 0.2692470837751856, Recl@10: 0.12457681173642117, NDCG@10: 0.7369362820085145\n",
      "---------------------------------------------------\n",
      "Train | 68/100, BPR loss: 0.095496240234375\n",
      "Valid | 68/100, Pre@10: 0.2683987274655355, Recl@10: 0.12418428928904372, NDCG@10: 0.7370907427387932\n",
      "---------------------------------------------------\n",
      "Train | 69/100, BPR loss: 0.10156771850585937\n",
      "Valid | 69/100, Pre@10: 0.26617179215270415, Recl@10: 0.12315391786467789, NDCG@10: 0.735305453827252\n",
      "---------------------------------------------------\n",
      "Train | 70/100, BPR loss: 0.105404345703125\n",
      "Valid | 70/100, Pre@10: 0.26521739130434785, Recl@10: 0.12271233011137825, NDCG@10: 0.7353112955858914\n",
      "---------------------------------------------------\n",
      "Train | 71/100, BPR loss: 0.09633331909179688\n",
      "Valid | 71/100, Pre@10: 0.26702014846235417, Recl@10: 0.12354644031205535, NDCG@10: 0.7353648823409864\n",
      "---------------------------------------------------\n",
      "Train | 72/100, BPR loss: 0.10106792602539062\n",
      "Valid | 72/100, Pre@10: 0.26638388123011664, Recl@10: 0.12325204847652226, NDCG@10: 0.734827182889126\n",
      "---------------------------------------------------\n",
      "Train | 73/100, BPR loss: 0.09080488891601562\n",
      "Valid | 73/100, Pre@10: 0.26733828207847293, Recl@10: 0.1236936362298219, NDCG@10: 0.7349495796682461\n",
      "---------------------------------------------------\n",
      "Train | 74/100, BPR loss: 0.097009033203125\n",
      "Valid | 74/100, Pre@10: 0.26638388123011664, Recl@10: 0.12325204847652226, NDCG@10: 0.7363668602450898\n",
      "---------------------------------------------------\n",
      "Train | 75/100, BPR loss: 0.096312841796875\n",
      "Valid | 75/100, Pre@10: 0.2672322375397667, Recl@10: 0.12364457092389972, NDCG@10: 0.7363634808649244\n",
      "---------------------------------------------------\n",
      "Train | 76/100, BPR loss: 0.09392823486328125\n",
      "Valid | 76/100, Pre@10: 0.26617179215270415, Recl@10: 0.12315391786467789, NDCG@10: 0.7366677257353491\n",
      "---------------------------------------------------\n",
      "Train | 77/100, BPR loss: 0.094149951171875\n",
      "Valid | 77/100, Pre@10: 0.2650053022269353, Recl@10: 0.12261419949953388, NDCG@10: 0.737514626943905\n",
      "---------------------------------------------------\n",
      "Train | 78/100, BPR loss: 0.090841845703125\n",
      "Valid | 78/100, Pre@10: 0.26362672322375397, Recl@10: 0.1219763505225455, NDCG@10: 0.7387851917104654\n",
      "---------------------------------------------------\n",
      "Train | 79/100, BPR loss: 0.09902777099609375\n",
      "Valid | 79/100, Pre@10: 0.26203605514316014, Recl@10: 0.12124037093371277, NDCG@10: 0.7382180831833162\n",
      "---------------------------------------------------\n",
      "Train | 80/100, BPR loss: 0.09211803588867187\n",
      "Valid | 80/100, Pre@10: 0.26097560975609757, Recl@10: 0.12074971787449094, NDCG@10: 0.7392855682180849\n",
      "---------------------------------------------------\n",
      "Train | 81/100, BPR loss: 0.082049560546875\n",
      "Valid | 81/100, Pre@10: 0.26055143160127253, Recl@10: 0.12055345665080222, NDCG@10: 0.7403703250662427\n",
      "---------------------------------------------------\n",
      "Train | 82/100, BPR loss: 0.0938274658203125\n",
      "Valid | 82/100, Pre@10: 0.26118769883351006, Recl@10: 0.12084784848633531, NDCG@10: 0.7408498491633579\n",
      "---------------------------------------------------\n",
      "Train | 83/100, BPR loss: 0.08393856201171875\n",
      "Valid | 83/100, Pre@10: 0.26267232237539767, Recl@10: 0.12153476276924587, NDCG@10: 0.7417760467348203\n",
      "---------------------------------------------------\n",
      "Train | 84/100, BPR loss: 0.08585143432617187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid | 84/100, Pre@10: 0.25949098621420996, Recl@10: 0.12006280359158039, NDCG@10: 0.7424279925506984\n",
      "---------------------------------------------------\n",
      "Train | 85/100, BPR loss: 0.09132813720703126\n",
      "Valid | 85/100, Pre@10: 0.25705196182396606, Recl@10: 0.1189343015553702, NDCG@10: 0.7424149198976535\n",
      "---------------------------------------------------\n",
      "Train | 86/100, BPR loss: 0.08782440185546875\n",
      "Valid | 86/100, Pre@10: 0.25885471898197243, Recl@10: 0.1197684117560473, NDCG@10: 0.7433990462119722\n",
      "---------------------------------------------------\n",
      "Train | 87/100, BPR loss: 0.08135389404296875\n",
      "Valid | 87/100, Pre@10: 0.2569459172852598, Recl@10: 0.11888523624944801, NDCG@10: 0.741515400371564\n",
      "---------------------------------------------------\n",
      "Train | 88/100, BPR loss: 0.08995355224609375\n",
      "Valid | 88/100, Pre@10: 0.2598091198303287, Recl@10: 0.12020999950934694, NDCG@10: 0.7421071929620879\n",
      "---------------------------------------------------\n",
      "Train | 89/100, BPR loss: 0.0894281005859375\n",
      "Valid | 89/100, Pre@10: 0.25864262990455994, Recl@10: 0.11967028114420293, NDCG@10: 0.7422725058387286\n",
      "---------------------------------------------------\n",
      "Train | 90/100, BPR loss: 0.0826647705078125\n",
      "Valid | 90/100, Pre@10: 0.25853658536585367, Recl@10: 0.11962121583828075, NDCG@10: 0.7425461706351278\n",
      "---------------------------------------------------\n",
      "Train | 91/100, BPR loss: 0.09382413940429687\n",
      "Valid | 91/100, Pre@10: 0.2582184517497349, Recl@10: 0.11947401992051421, NDCG@10: 0.7417956740230836\n",
      "---------------------------------------------------\n",
      "Train | 92/100, BPR loss: 0.08718827514648438\n",
      "Valid | 92/100, Pre@10: 0.2576882290562036, Recl@10: 0.11922869339090329, NDCG@10: 0.7403305234229702\n",
      "---------------------------------------------------\n",
      "Train | 93/100, BPR loss: 0.08667958984375\n",
      "Valid | 93/100, Pre@10: 0.25705196182396606, Recl@10: 0.1189343015553702, NDCG@10: 0.739959803863076\n",
      "---------------------------------------------------\n",
      "Train | 94/100, BPR loss: 0.08533179931640625\n",
      "Valid | 94/100, Pre@10: 0.2572640509013786, Recl@10: 0.11903243216721457, NDCG@10: 0.7403294716742249\n",
      "---------------------------------------------------\n",
      "Train | 95/100, BPR loss: 0.08600883178710937\n",
      "Valid | 95/100, Pre@10: 0.2573700954400848, Recl@10: 0.11908149747313675, NDCG@10: 0.7413298792848945\n",
      "---------------------------------------------------\n",
      "Train | 96/100, BPR loss: 0.08593619384765624\n",
      "Valid | 96/100, Pre@10: 0.25885471898197243, Recl@10: 0.1197684117560473, NDCG@10: 0.7419607670746342\n",
      "---------------------------------------------------\n",
      "Train | 97/100, BPR loss: 0.08279829711914062\n",
      "Valid | 97/100, Pre@10: 0.2572640509013786, Recl@10: 0.11903243216721457, NDCG@10: 0.741516151252351\n",
      "---------------------------------------------------\n",
      "Train | 98/100, BPR loss: 0.07417176513671875\n",
      "Valid | 98/100, Pre@10: 0.2548250265111347, Recl@10: 0.11790393013100436, NDCG@10: 0.7409647520487753\n",
      "---------------------------------------------------\n",
      "Train | 99/100, BPR loss: 0.08614288330078125\n",
      "Valid | 99/100, Pre@10: 0.2549310710498409, Recl@10: 0.11795299543692656, NDCG@10: 0.741067889239409\n",
      "---------------------------------------------------\n",
      "Train | 100/100, BPR loss: 0.08530430908203125\n",
      "Valid | 100/100, Pre@10: 0.25376458112407213, Recl@10: 0.11741327707178255, NDCG@10: 0.7422550160233727\n",
      "---------------------------------------------------\n",
      "CPU times: total: 3min 15s\n",
      "Wall time: 7min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model1.fit(k = 50, max_iter = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88834487",
   "metadata": {},
   "source": [
    "### 1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49eca292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BPR()\n",
    "model2.preprocess(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2969e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 1/100, BPR loss: 0.6931466796875\n",
      "Valid | 1/100, Pre@10: 0.016109271523178807, Recl@10: 0.004806101229433295, NDCG@10: 0.6685183457893602\n",
      "---------------------------------------------------\n",
      "Train | 2/100, BPR loss: 0.68762822265625\n",
      "Valid | 2/100, Pre@10: 0.06372516556291391, Recl@10: 0.019012007843873333, NDCG@10: 0.7081679037241245\n",
      "---------------------------------------------------\n",
      "Train | 3/100, BPR loss: 0.656017578125\n",
      "Valid | 3/100, Pre@10: 0.10892384105960265, Recl@10: 0.03249675230055668, NDCG@10: 0.7281154896414167\n",
      "---------------------------------------------------\n",
      "Train | 4/100, BPR loss: 0.60083056640625\n",
      "Valid | 4/100, Pre@10: 0.12693708609271523, Recl@10: 0.037870892215894215, NDCG@10: 0.7381618853583445\n",
      "---------------------------------------------------\n",
      "Train | 5/100, BPR loss: 0.52819755859375\n",
      "Valid | 5/100, Pre@10: 0.13905629139072848, Recl@10: 0.041486581938345576, NDCG@10: 0.7456189506339896\n",
      "---------------------------------------------------\n",
      "Train | 6/100, BPR loss: 0.46099462890625\n",
      "Valid | 6/100, Pre@10: 0.14736754966887416, Recl@10: 0.043966194288988444, NDCG@10: 0.7501278895887448\n",
      "---------------------------------------------------\n",
      "Train | 7/100, BPR loss: 0.41416728515625\n",
      "Valid | 7/100, Pre@10: 0.15966887417218542, Recl@10: 0.04763621814661326, NDCG@10: 0.7536632242545317\n",
      "---------------------------------------------------\n",
      "Train | 8/100, BPR loss: 0.3798757568359375\n",
      "Valid | 8/100, Pre@10: 0.16071192052980132, Recl@10: 0.0479474045571521, NDCG@10: 0.7540410072131019\n",
      "---------------------------------------------------\n",
      "Train | 9/100, BPR loss: 0.357935546875\n",
      "Valid | 9/100, Pre@10: 0.163841059602649, Recl@10: 0.04888096378876864, NDCG@10: 0.7548050365622868\n",
      "---------------------------------------------------\n",
      "Train | 10/100, BPR loss: 0.35896162109375\n",
      "Valid | 10/100, Pre@10: 0.1656953642384106, Recl@10: 0.049434184074171036, NDCG@10: 0.7551103452401148\n",
      "---------------------------------------------------\n",
      "Train | 11/100, BPR loss: 0.3664998046875\n",
      "Valid | 11/100, Pre@10: 0.1678476821192053, Recl@10: 0.05007631476258453, NDCG@10: 0.7538696717279675\n",
      "---------------------------------------------------\n",
      "Train | 12/100, BPR loss: 0.373203125\n",
      "Valid | 12/100, Pre@10: 0.16562913907284768, Recl@10: 0.049414426206835234, NDCG@10: 0.7524301511586653\n",
      "---------------------------------------------------\n",
      "Train | 13/100, BPR loss: 0.3702674560546875\n",
      "Valid | 13/100, Pre@10: 0.16304635761589403, Recl@10: 0.048643869380739044, NDCG@10: 0.7514894756362787\n",
      "---------------------------------------------------\n",
      "Train | 14/100, BPR loss: 0.390301953125\n",
      "Valid | 14/100, Pre@10: 0.1626158940397351, Recl@10: 0.048515443243056344, NDCG@10: 0.7512425803532161\n",
      "---------------------------------------------------\n",
      "Train | 15/100, BPR loss: 0.412335205078125\n",
      "Valid | 15/100, Pre@10: 0.1639403973509934, Recl@10: 0.04891060058977234, NDCG@10: 0.7505921707243527\n",
      "---------------------------------------------------\n",
      "Train | 16/100, BPR loss: 0.410262744140625\n",
      "Valid | 16/100, Pre@10: 0.1643046357615894, Recl@10: 0.04901926886011924, NDCG@10: 0.7512640713351988\n",
      "---------------------------------------------------\n",
      "Train | 17/100, BPR loss: 0.387735009765625\n",
      "Valid | 17/100, Pre@10: 0.16355960264900662, Recl@10: 0.04879699285259149, NDCG@10: 0.7508067778521834\n",
      "---------------------------------------------------\n",
      "Train | 18/100, BPR loss: 0.406234521484375\n",
      "Valid | 18/100, Pre@10: 0.16245033112582782, Recl@10: 0.04846604857471685, NDCG@10: 0.7496859168866346\n",
      "---------------------------------------------------\n",
      "Train | 19/100, BPR loss: 0.3894095458984375\n",
      "Valid | 19/100, Pre@10: 0.16102649006622516, Recl@10: 0.04804125442699715, NDCG@10: 0.7483008549606062\n",
      "---------------------------------------------------\n",
      "Train | 20/100, BPR loss: 0.4058326416015625\n",
      "Valid | 20/100, Pre@10: 0.16096026490066226, Recl@10: 0.04802149655966135, NDCG@10: 0.7475014686932069\n",
      "---------------------------------------------------\n",
      "Train | 21/100, BPR loss: 0.390331494140625\n",
      "Valid | 21/100, Pre@10: 0.16155629139072847, Recl@10: 0.04819931736568355, NDCG@10: 0.745699763037977\n",
      "---------------------------------------------------\n",
      "Train | 22/100, BPR loss: 0.385784814453125\n",
      "Valid | 22/100, Pre@10: 0.1614403973509934, Recl@10: 0.048164741097845895, NDCG@10: 0.7447667175894166\n",
      "---------------------------------------------------\n",
      "Train | 23/100, BPR loss: 0.371289306640625\n",
      "Valid | 23/100, Pre@10: 0.15730132450331125, Recl@10: 0.04692987438935841, NDCG@10: 0.742810640286488\n",
      "---------------------------------------------------\n",
      "Train | 24/100, BPR loss: 0.3705201171875\n",
      "Valid | 24/100, Pre@10: 0.15349337748344372, Recl@10: 0.045793797017549924, NDCG@10: 0.7411850514789832\n",
      "---------------------------------------------------\n",
      "Train | 25/100, BPR loss: 0.3669093994140625\n",
      "Valid | 25/100, Pre@10: 0.14783112582781457, Recl@10: 0.04410449936033904, NDCG@10: 0.7407025178916624\n",
      "---------------------------------------------------\n",
      "Train | 26/100, BPR loss: 0.37330322265625\n",
      "Valid | 26/100, Pre@10: 0.14289735099337747, Recl@10: 0.04263253824382196, NDCG@10: 0.7400276912828945\n",
      "---------------------------------------------------\n",
      "Train | 27/100, BPR loss: 0.3748908203125\n",
      "Valid | 27/100, Pre@10: 0.13890728476821193, Recl@10: 0.04144212673684002, NDCG@10: 0.7394821573681979\n",
      "---------------------------------------------------\n",
      "Train | 28/100, BPR loss: 0.3658769287109375\n",
      "Valid | 28/100, Pre@10: 0.13281456953642384, Recl@10: 0.03962440294194645, NDCG@10: 0.7387042197556145\n",
      "---------------------------------------------------\n",
      "Train | 29/100, BPR loss: 0.34998876953125\n",
      "Valid | 29/100, Pre@10: 0.1231953642384106, Recl@10: 0.03675457271142153, NDCG@10: 0.7378480652887343\n",
      "---------------------------------------------------\n",
      "Train | 30/100, BPR loss: 0.369023779296875\n",
      "Valid | 30/100, Pre@10: 0.11652317880794702, Recl@10: 0.034763967577339704, NDCG@10: 0.7366759308814802\n",
      "---------------------------------------------------\n",
      "Train | 31/100, BPR loss: 0.36731572265625\n",
      "Valid | 31/100, Pre@10: 0.1104635761589404, Recl@10: 0.03295612271611402, NDCG@10: 0.7351131545353633\n",
      "---------------------------------------------------\n",
      "Train | 32/100, BPR loss: 0.3493809326171875\n",
      "Valid | 32/100, Pre@10: 0.1089569536423841, Recl@10: 0.03250663123422458, NDCG@10: 0.7345257039882351\n",
      "---------------------------------------------------\n",
      "Train | 33/100, BPR loss: 0.34088232421875\n",
      "Valid | 33/100, Pre@10: 0.10763245033112583, Recl@10: 0.03211147388750858, NDCG@10: 0.734738820002758\n",
      "---------------------------------------------------\n",
      "Train | 34/100, BPR loss: 0.334964990234375\n",
      "Valid | 34/100, Pre@10: 0.10509933774834437, Recl@10: 0.03135573546191424, NDCG@10: 0.7346237637694903\n",
      "---------------------------------------------------\n",
      "Train | 35/100, BPR loss: 0.3411978759765625\n",
      "Valid | 35/100, Pre@10: 0.10632450331125828, Recl@10: 0.031721256007626535, NDCG@10: 0.735426005042197\n",
      "---------------------------------------------------\n",
      "Train | 36/100, BPR loss: 0.3319761474609375\n",
      "Valid | 36/100, Pre@10: 0.11024834437086092, Recl@10: 0.03289190964727268, NDCG@10: 0.736690299714251\n",
      "---------------------------------------------------\n",
      "Train | 37/100, BPR loss: 0.3406588134765625\n",
      "Valid | 37/100, Pre@10: 0.11632450331125828, Recl@10: 0.0347046939753323, NDCG@10: 0.7378625988772989\n",
      "---------------------------------------------------\n",
      "Train | 38/100, BPR loss: 0.33918076171875\n",
      "Valid | 38/100, Pre@10: 0.1210430463576159, Recl@10: 0.036112442023008035, NDCG@10: 0.7380303458488747\n",
      "---------------------------------------------------\n",
      "Train | 39/100, BPR loss: 0.3357387451171875\n",
      "Valid | 39/100, Pre@10: 0.12470198675496688, Recl@10: 0.03720406419331097, NDCG@10: 0.7385495423038533\n",
      "---------------------------------------------------\n",
      "Train | 40/100, BPR loss: 0.336086865234375\n",
      "Valid | 40/100, Pre@10: 0.12743377483443707, Recl@10: 0.03801907622091272, NDCG@10: 0.7400162049681581\n",
      "---------------------------------------------------\n",
      "Train | 41/100, BPR loss: 0.33137109375\n",
      "Valid | 41/100, Pre@10: 0.13091059602649008, Recl@10: 0.039056364256042204, NDCG@10: 0.7405241925760913\n",
      "---------------------------------------------------\n",
      "Train | 42/100, BPR loss: 0.3143439453125\n",
      "Valid | 42/100, Pre@10: 0.1318046357615894, Recl@10: 0.0393230954650755, NDCG@10: 0.7416701183120681\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 43/100, BPR loss: 0.308310595703125\n",
      "Valid | 43/100, Pre@10: 0.13208609271523178, Recl@10: 0.03940706640125265, NDCG@10: 0.7426009804482531\n",
      "---------------------------------------------------\n",
      "Train | 44/100, BPR loss: 0.318236962890625\n",
      "Valid | 44/100, Pre@10: 0.13458609271523178, Recl@10: 0.04015292589317909, NDCG@10: 0.7427478563576005\n",
      "---------------------------------------------------\n",
      "Train | 45/100, BPR loss: 0.313182177734375\n",
      "Valid | 45/100, Pre@10: 0.13605960264900663, Recl@10: 0.040592538441400636, NDCG@10: 0.7436070919590209\n",
      "---------------------------------------------------\n",
      "Train | 46/100, BPR loss: 0.306265625\n",
      "Valid | 46/100, Pre@10: 0.13716887417218543, Recl@10: 0.04092348271927528, NDCG@10: 0.743810729570939\n",
      "---------------------------------------------------\n",
      "Train | 47/100, BPR loss: 0.3026563720703125\n",
      "Valid | 47/100, Pre@10: 0.13844370860927152, Recl@10: 0.041303821665489425, NDCG@10: 0.7441601850494735\n",
      "---------------------------------------------------\n",
      "Train | 48/100, BPR loss: 0.3037255859375\n",
      "Valid | 48/100, Pre@10: 0.1396026490066225, Recl@10: 0.041649584343865925, NDCG@10: 0.7449639053891159\n",
      "---------------------------------------------------\n",
      "Train | 49/100, BPR loss: 0.295690185546875\n",
      "Valid | 49/100, Pre@10: 0.1419205298013245, Recl@10: 0.04234110970061891, NDCG@10: 0.7460696764883182\n",
      "---------------------------------------------------\n",
      "Train | 50/100, BPR loss: 0.29690185546875\n",
      "Valid | 50/100, Pre@10: 0.14410596026490066, Recl@10: 0.04299311932270031, NDCG@10: 0.7461497416684114\n",
      "---------------------------------------------------\n",
      "Train | 51/100, BPR loss: 0.288745947265625\n",
      "Valid | 51/100, Pre@10: 0.1452980132450331, Recl@10: 0.043348760934744705, NDCG@10: 0.7460444445918015\n",
      "---------------------------------------------------\n",
      "Train | 52/100, BPR loss: 0.2886400634765625\n",
      "Valid | 52/100, Pre@10: 0.14746688741721856, Recl@10: 0.043995831089992145, NDCG@10: 0.7458702998180602\n",
      "---------------------------------------------------\n",
      "Train | 53/100, BPR loss: 0.2926634765625\n",
      "Valid | 53/100, Pre@10: 0.14887417218543048, Recl@10: 0.04441568577087789, NDCG@10: 0.7458723348008823\n",
      "---------------------------------------------------\n",
      "Train | 54/100, BPR loss: 0.283512255859375\n",
      "Valid | 54/100, Pre@10: 0.15168874172185431, Recl@10: 0.04525539513264938, NDCG@10: 0.7458812842841119\n",
      "---------------------------------------------------\n",
      "Train | 55/100, BPR loss: 0.2706468505859375\n",
      "Valid | 55/100, Pre@10: 0.15367549668874173, Recl@10: 0.045848131152723376, NDCG@10: 0.7454107186862403\n",
      "---------------------------------------------------\n",
      "Train | 56/100, BPR loss: 0.265037060546875\n",
      "Valid | 56/100, Pre@10: 0.15437086092715233, Recl@10: 0.04605558875974927, NDCG@10: 0.7447658733220152\n",
      "---------------------------------------------------\n",
      "Train | 57/100, BPR loss: 0.2846094970703125\n",
      "Valid | 57/100, Pre@10: 0.1570364238410596, Recl@10: 0.046850842920015213, NDCG@10: 0.7433634193912156\n",
      "---------------------------------------------------\n",
      "Train | 58/100, BPR loss: 0.276860888671875\n",
      "Valid | 58/100, Pre@10: 0.1566887417218543, Recl@10: 0.046747114116502266, NDCG@10: 0.742874070669529\n",
      "---------------------------------------------------\n",
      "Train | 59/100, BPR loss: 0.264770849609375\n",
      "Valid | 59/100, Pre@10: 0.15619205298013245, Recl@10: 0.04659893011148376, NDCG@10: 0.7417211252857931\n",
      "---------------------------------------------------\n",
      "Train | 60/100, BPR loss: 0.268103125\n",
      "Valid | 60/100, Pre@10: 0.15451986754966887, Recl@10: 0.04610004396125482, NDCG@10: 0.74069962308933\n",
      "---------------------------------------------------\n",
      "Train | 61/100, BPR loss: 0.262285693359375\n",
      "Valid | 61/100, Pre@10: 0.15293046357615894, Recl@10: 0.045625855145195626, NDCG@10: 0.7393466273743713\n",
      "---------------------------------------------------\n",
      "Train | 62/100, BPR loss: 0.268261962890625\n",
      "Valid | 62/100, Pre@10: 0.152317880794702, Recl@10: 0.04544309487233948, NDCG@10: 0.7384731125934281\n",
      "---------------------------------------------------\n",
      "Train | 63/100, BPR loss: 0.264587744140625\n",
      "Valid | 63/100, Pre@10: 0.1526655629139073, Recl@10: 0.04554682367585243, NDCG@10: 0.7376491618015784\n",
      "---------------------------------------------------\n",
      "Train | 64/100, BPR loss: 0.2610246337890625\n",
      "Valid | 64/100, Pre@10: 0.15193708609271522, Recl@10: 0.04532948713515863, NDCG@10: 0.7364811569377043\n",
      "---------------------------------------------------\n",
      "Train | 65/100, BPR loss: 0.2558003173828125\n",
      "Valid | 65/100, Pre@10: 0.15221854304635762, Recl@10: 0.04541345807133578, NDCG@10: 0.7349141939112003\n",
      "---------------------------------------------------\n",
      "Train | 66/100, BPR loss: 0.2546029296875\n",
      "Valid | 66/100, Pre@10: 0.1524337748344371, Recl@10: 0.04547767114017713, NDCG@10: 0.7336922799566192\n",
      "---------------------------------------------------\n",
      "Train | 67/100, BPR loss: 0.2558572021484375\n",
      "Valid | 67/100, Pre@10: 0.15206953642384105, Recl@10: 0.04536900286983023, NDCG@10: 0.7323589491705568\n",
      "---------------------------------------------------\n",
      "Train | 68/100, BPR loss: 0.25158076171875\n",
      "Valid | 68/100, Pre@10: 0.15271523178807947, Recl@10: 0.045561642076354276, NDCG@10: 0.7318861107620283\n",
      "---------------------------------------------------\n",
      "Train | 69/100, BPR loss: 0.250108251953125\n",
      "Valid | 69/100, Pre@10: 0.15503311258278146, Recl@10: 0.04625316743310727, NDCG@10: 0.7320741083475869\n",
      "---------------------------------------------------\n",
      "Train | 70/100, BPR loss: 0.2372861328125\n",
      "Valid | 70/100, Pre@10: 0.1576158940397351, Recl@10: 0.04702372425920346, NDCG@10: 0.7314215732066974\n",
      "---------------------------------------------------\n",
      "Train | 71/100, BPR loss: 0.247697216796875\n",
      "Valid | 71/100, Pre@10: 0.16024834437086094, Recl@10: 0.047809099485801504, NDCG@10: 0.7316393529716507\n",
      "---------------------------------------------------\n",
      "Train | 72/100, BPR loss: 0.24344501953125\n",
      "Valid | 72/100, Pre@10: 0.16245033112582782, Recl@10: 0.04846604857471685, NDCG@10: 0.7315873442005094\n",
      "---------------------------------------------------\n",
      "Train | 73/100, BPR loss: 0.23705927734375\n",
      "Valid | 73/100, Pre@10: 0.16461920529801324, Recl@10: 0.04911311872996429, NDCG@10: 0.7311829663173874\n",
      "---------------------------------------------------\n",
      "Train | 74/100, BPR loss: 0.2373103515625\n",
      "Valid | 74/100, Pre@10: 0.16572847682119204, Recl@10: 0.049444063007838934, NDCG@10: 0.7310975398573591\n",
      "---------------------------------------------------\n",
      "Train | 75/100, BPR loss: 0.231939599609375\n",
      "Valid | 75/100, Pre@10: 0.1675496688741722, Recl@10: 0.049987404359573426, NDCG@10: 0.7306201289732899\n",
      "---------------------------------------------------\n",
      "Train | 76/100, BPR loss: 0.22390517578125\n",
      "Valid | 76/100, Pre@10: 0.16956953642384107, Recl@10: 0.05059001931331532, NDCG@10: 0.7306003591778959\n",
      "---------------------------------------------------\n",
      "Train | 77/100, BPR loss: 0.23293779296875\n",
      "Valid | 77/100, Pre@10: 0.17155629139072848, Recl@10: 0.05118275533338931, NDCG@10: 0.7304740759525885\n",
      "---------------------------------------------------\n",
      "Train | 78/100, BPR loss: 0.2325729248046875\n",
      "Valid | 78/100, Pre@10: 0.17298013245033111, Recl@10: 0.05160754948110901, NDCG@10: 0.7307238129710065\n",
      "---------------------------------------------------\n",
      "Train | 79/100, BPR loss: 0.2307126708984375\n",
      "Valid | 79/100, Pre@10: 0.1727980132450331, Recl@10: 0.05155321534593556, NDCG@10: 0.7308520562919093\n",
      "---------------------------------------------------\n",
      "Train | 80/100, BPR loss: 0.230742578125\n",
      "Valid | 80/100, Pre@10: 0.17369205298013246, Recl@10: 0.051819946554968854, NDCG@10: 0.7311276700799942\n",
      "---------------------------------------------------\n",
      "Train | 81/100, BPR loss: 0.22253935546875\n",
      "Valid | 81/100, Pre@10: 0.1746026490066225, Recl@10: 0.05209161723083611, NDCG@10: 0.7308255098989094\n",
      "---------------------------------------------------\n",
      "Train | 82/100, BPR loss: 0.23087880859375\n",
      "Valid | 82/100, Pre@10: 0.17473509933774833, Recl@10: 0.052131132965507705, NDCG@10: 0.7309292118557849\n",
      "---------------------------------------------------\n",
      "Train | 83/100, BPR loss: 0.223588134765625\n",
      "Valid | 83/100, Pre@10: 0.1752980132450331, Recl@10: 0.052299074837862, NDCG@10: 0.7312946650514806\n",
      "---------------------------------------------------\n",
      "Train | 84/100, BPR loss: 0.227995849609375\n",
      "Valid | 84/100, Pre@10: 0.17662251655629138, Recl@10: 0.052694232184578, NDCG@10: 0.7310746322865197\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 85/100, BPR loss: 0.222470654296875\n",
      "Valid | 85/100, Pre@10: 0.1770860927152318, Recl@10: 0.0528325372559286, NDCG@10: 0.7315264421631569\n",
      "---------------------------------------------------\n",
      "Train | 86/100, BPR loss: 0.220691015625\n",
      "Valid | 86/100, Pre@10: 0.17875827814569537, Recl@10: 0.05333142340615754, NDCG@10: 0.7319085590159774\n",
      "---------------------------------------------------\n",
      "Train | 87/100, BPR loss: 0.2126048828125\n",
      "Valid | 87/100, Pre@10: 0.18082781456953642, Recl@10: 0.05394885676040128, NDCG@10: 0.7315499824171668\n",
      "---------------------------------------------------\n",
      "Train | 88/100, BPR loss: 0.224698095703125\n",
      "Valid | 88/100, Pre@10: 0.18228476821192052, Recl@10: 0.054383529841788875, NDCG@10: 0.7315228056851971\n",
      "---------------------------------------------------\n",
      "Train | 89/100, BPR loss: 0.2110837158203125\n",
      "Valid | 89/100, Pre@10: 0.18329470198675496, Recl@10: 0.05468483731865982, NDCG@10: 0.7314029500003244\n",
      "---------------------------------------------------\n",
      "Train | 90/100, BPR loss: 0.230044140625\n",
      "Valid | 90/100, Pre@10: 0.1829635761589404, Recl@10: 0.05458604798198082, NDCG@10: 0.7312204436708081\n",
      "---------------------------------------------------\n",
      "Train | 91/100, BPR loss: 0.210469580078125\n",
      "Valid | 91/100, Pre@10: 0.18390728476821191, Recl@10: 0.05486759759151597, NDCG@10: 0.7313622046078834\n",
      "---------------------------------------------------\n",
      "Train | 92/100, BPR loss: 0.2178586181640625\n",
      "Valid | 92/100, Pre@10: 0.18466887417218544, Recl@10: 0.05509481306587767, NDCG@10: 0.7309139982514309\n",
      "---------------------------------------------------\n",
      "Train | 93/100, BPR loss: 0.218260888671875\n",
      "Valid | 93/100, Pre@10: 0.1858609271523179, Recl@10: 0.05545045467792206, NDCG@10: 0.7310086596779322\n",
      "---------------------------------------------------\n",
      "Train | 94/100, BPR loss: 0.207585986328125\n",
      "Valid | 94/100, Pre@10: 0.18637417218543045, Recl@10: 0.05560357814977451, NDCG@10: 0.7308659182220242\n",
      "---------------------------------------------------\n",
      "Train | 95/100, BPR loss: 0.2153830322265625\n",
      "Valid | 95/100, Pre@10: 0.18811258278145696, Recl@10: 0.05612222216733926, NDCG@10: 0.7303636550343263\n",
      "---------------------------------------------------\n",
      "Train | 96/100, BPR loss: 0.207322119140625\n",
      "Valid | 96/100, Pre@10: 0.18794701986754966, Recl@10: 0.05607282749899976, NDCG@10: 0.7304595775345838\n",
      "---------------------------------------------------\n",
      "Train | 97/100, BPR loss: 0.213213525390625\n",
      "Valid | 97/100, Pre@10: 0.18758278145695365, Recl@10: 0.05596415922865286, NDCG@10: 0.7303859119391386\n",
      "---------------------------------------------------\n",
      "Train | 98/100, BPR loss: 0.2130035888671875\n",
      "Valid | 98/100, Pre@10: 0.18740066225165564, Recl@10: 0.05590982509347941, NDCG@10: 0.7303450945769724\n",
      "---------------------------------------------------\n",
      "Train | 99/100, BPR loss: 0.2193486083984375\n",
      "Valid | 99/100, Pre@10: 0.18721854304635763, Recl@10: 0.05585549095830596, NDCG@10: 0.7295966928735188\n",
      "---------------------------------------------------\n",
      "Train | 100/100, BPR loss: 0.21443994140625\n",
      "Valid | 100/100, Pre@10: 0.18683774834437086, Recl@10: 0.05574188322112511, NDCG@10: 0.7286673493610379\n",
      "---------------------------------------------------\n",
      "CPU times: total: 21min 21s\n",
      "Wall time: 52min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model2.fit(k = 50, max_iter = 100, stepsize=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370fa4da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
