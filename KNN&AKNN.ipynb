{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34d707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from math import exp\n",
    "from math import log\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfde470",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f08131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_dir):\n",
    "\n",
    "    user_ids_dict, rated_item_ids_dict = {},{}\n",
    "    N, M, u_idx, i_idx = 0,0,0,0 \n",
    "    data = []\n",
    "    f = open(file_dir)\n",
    "    for line in f.readlines():\n",
    "        if '::' in line:\n",
    "            u, i, r, _ = line.split('::')\n",
    "        else:\n",
    "            u, i, r, _ = line.split()\n",
    "    \n",
    "        if int(u) not in user_ids_dict:\n",
    "            user_ids_dict[int(u)]=u_idx\n",
    "            u_idx+=1\n",
    "        if int(i) not in rated_item_ids_dict:\n",
    "            rated_item_ids_dict[int(i)]=i_idx\n",
    "            i_idx+=1\n",
    "        data.append([user_ids_dict[int(u)],rated_item_ids_dict[int(i)],float(r)])\n",
    "    \n",
    "    f.close()\n",
    "    N = u_idx\n",
    "    M = i_idx\n",
    "\n",
    "    return N, M, data, rated_item_ids_dict\n",
    "\n",
    "\n",
    "def sequence2mat(sequence, N, M):\n",
    "    \n",
    "    records_array = np.array(sequence)\n",
    "    mat = np.zeros([N,M])\n",
    "    row = records_array[:,0].astype(int)\n",
    "    col = records_array[:,1].astype(int)\n",
    "    values = records_array[:,2].astype(np.float32)\n",
    "    mat[row,col]=values\n",
    "    \n",
    "    return mat\n",
    "\n",
    "\n",
    "def calculate_similarity(a, b, model='pearson', minimum_common_items=5):\n",
    "    assert a.shape==b.shape\n",
    "    dim = len(a.shape) #向量维度\n",
    "    common_items = a*b>0 # 共同评分的项\n",
    "    common_size = np.sum(common_items,axis=dim-1)\n",
    "    \n",
    "    if model=='pearson':\n",
    "        mean_a = np.sum(a,axis=dim-1)/np.sum(a>0,axis=dim-1)\n",
    "        mean_b = np.sum(b,axis=dim-1)/np.sum(b>0,axis=dim-1)\n",
    "        if dim ==1: #若是两个列向量\n",
    "            aa = (a - mean_a)*common_items\n",
    "            bb = (b - mean_b)*common_items\n",
    "        else:\n",
    "            aa = (a - np.reshape(mean_a, (-1,1)))*common_items\n",
    "            bb = (b - np.reshape(mean_b, (-1,1)))*common_items\n",
    "    else: #consine\n",
    "        mean_u = np.sum(b,axis=0)/np.sum(b>0,axis=0)\n",
    "        aa = (a - mean_u)*common_items\n",
    "        bb = (b - mean_u)*common_items\n",
    "        \n",
    "    sim = np.sum(aa*bb, axis=dim-1)/(np.sqrt(np.sum(aa**2, axis=dim-1))*np.sqrt(np.sum(bb**2, axis=dim-1)) + 1e-10)\n",
    "    least_common_items = common_size>minimum_common_items # 共同评分的商品不少于least_common_items\n",
    "    return sim*least_common_items\n",
    "\n",
    "\n",
    "def similarity_matrix(mat, model='pearson', minimum_common_items=5):\n",
    "    n,m = mat.shape\n",
    "    sim_list=[]\n",
    "    for u in range(n):\n",
    "        a = np.tile(mat[u,:], (n,1))\n",
    "        b = mat\n",
    "        if model=='pearson':\n",
    "            sim = calculate_similarity(a, b, model='pearson', minimum_common_items=minimum_common_items)\n",
    "        else: # consine\n",
    "            sim = calculate_similarity(a, b, model='consine', minimum_common_items=minimum_common_items)\n",
    "        sim_list.append(sim)\n",
    "        if u % 100 ==0:\n",
    "            print(u)\n",
    "    return np.array(sim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a797ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data length: 1000209 \n",
      " user number: 6040 \n",
      " item number: 3706\n"
     ]
    }
   ],
   "source": [
    "file_dir = \"./ml-1m/ratings.dat\"      # \"./ml-100k/u.data\"\n",
    "\n",
    "N, M, data_list, item_ids_dict = load_data(file_dir=file_dir )\n",
    "print(' data length: %d \\n user number: %d \\n item number: %d' %(len(data_list),N,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990830c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\32698\\AppData\\Local\\Temp\\ipykernel_2188\\2262155707.py:56: RuntimeWarning: invalid value encountered in true_divide\n",
      "  mean_u = np.sum(b,axis=0)/np.sum(b>0,axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "train_list, test_list = train_test_split(data_list,test_size=0.2)\n",
    "\n",
    "train_mat = sequence2mat(sequence = train_list, N = N, M = M)\n",
    "test_mat = sequence2mat(sequence = test_list, N = N, M = M)\n",
    "\n",
    "sim_mat_pearson = similarity_matrix(mat=train_mat, model='pearson')\n",
    "neighbors_pearson = np.argsort(-np.array(sim_mat_pearson)) # 获取近邻\n",
    "sim_sort_pearson = -1*np.sort(-np.array(sim_mat_pearson)) # 获取对应近邻的相似度\n",
    "sim_mat_normal = similarity_matrix(mat=train_mat, model='123645')\n",
    "neighbors_normal = np.argsort(-np.array(sim_mat_normal)) # 获取近邻\n",
    "sim_sort_normal = -1*np.sort(-np.array(sim_mat_normal)) # 获取对应近邻的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435651ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_K(sim_mat, min_similarity=0.5):\n",
    "    num = np.sum(sim_mat[:,1:]>min_similarity, axis=1)     \n",
    "    num_sort = np.sort(-num)\n",
    "    line = int(0.8*len(sim_mat))\n",
    "    K = -1*num_sort[line]\n",
    "    return K\n",
    "\n",
    "def prediction(train_mat, sim_mat, K=1, model='user_based'):\n",
    "    assert len(train_mat.shape)>1\n",
    "    n,m = train_mat.shape\n",
    "    \n",
    "    if  model=='user_based':\n",
    "        sim_sort = -1*np.sort(-np.array(sim_mat))[:,1:K+1] # 除去最相似的自己\n",
    "        neighbors = np.argsort(-np.array(sim_mat))[:,1:K+1]\n",
    "        common_items = train_mat[neighbors]>0 \n",
    "        mean_user = np.reshape(np.sum(train_mat,axis=1)/np.sum(train_mat>0,axis=1), (-1,1))\n",
    "        mat_m = train_mat - mean_user\n",
    "        aa = np.sum(sim_sort[:,:,np.newaxis]*mat_m[neighbors]*common_items,axis=1)\n",
    "        bb = np.sum(sim_sort[:,:,np.newaxis]*common_items,axis=1)+1e-10 # 1e-10保证分母不为０\n",
    "        r_pred = mean_user + aa/bb\n",
    "        return r_pred\n",
    "    else: # 'item_based'\n",
    "        r_pred=[]\n",
    "        for u in range(n):\n",
    "            u_mat = np.tile(train_mat[u],(m,1)) # m份用户u的记录,m*m\n",
    "            rated_items_sim = (u_mat>0)*sim_mat # 保留有评分记录的相似度 m*m\n",
    "            sim_sort = -1*np.sort(-np.array(rated_items_sim))[:,:K] # m*K\n",
    "            neighbors = np.argsort(-np.array(rated_items_sim))[:,:K] # m*K\n",
    "            neighbor_ratings = np.array([u_mat[i,neighbors[i]] for i in range(m)])# m*K\n",
    "            aa = np.sum(sim_sort*neighbor_ratings,axis=1) # m*1\n",
    "            bb = np.sum(sim_sort,axis=1)+1e-10 # 1e-10保证分母不为０ m*1\n",
    "            r_pred.append(aa/bb)\n",
    "        \n",
    "        return np.array(r_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd0ca2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=5\n",
    "r_pred = prediction(train_mat=train_mat, sim_mat=sim_mat_pearson, K=K, model='user_based')\n",
    "r_pred_cos = prediction(train_mat=train_mat, sim_mat=sim_mat_normal, K=K, model='user_based')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa04da93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson recall:0.0077; precision:0.0510\n",
      "Cosine recall:0.0077; precision:0.0510\n"
     ]
    }
   ],
   "source": [
    "def get_dcg(ranklist, relevant_items):\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(ranklist):\n",
    "        if item in relevant_items:\n",
    "            dcg += 1 / math.log2(i + 2)\n",
    "    return dcg\n",
    "\n",
    "def get_idcg(relevant_items):\n",
    "    idcg = 0.0\n",
    "    for i in range(len(relevant_items)):\n",
    "        idcg += 1 / math.log2(i + 2)\n",
    "    return idcg\n",
    "\n",
    "def get_ndcg(ranklist, relevant_items):\n",
    "    dcg = get_dcg(ranklist, relevant_items)\n",
    "    idcg = get_idcg(relevant_items)\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "def calculate_ndcg_for_arrays(ranklists, relevant_items_sets):\n",
    "    ndcgs = []\n",
    "    for ranklist, relevant_items in zip(ranklists, relevant_items_sets):\n",
    "        ndcgs.append(get_ndcg(ranklist, set(relevant_items)))\n",
    "    return ndcgs\n",
    "\n",
    "def recall_precision(topn, test_mat):\n",
    "    n,m = test_mat.shape\n",
    "    hits,total_pred,total_true = 0.,0.,0.\n",
    "    for u in range(n):\n",
    "        hits += len([i for i in topn[u,:] if test_mat[u,i]>0])\n",
    "        size_pred = len(topn[u,:])\n",
    "        size_true = np.sum(test_mat[u,:]>0,axis=0)\n",
    "        total_pred += size_pred\n",
    "        total_true += size_true\n",
    "\n",
    "    recall = hits/total_true\n",
    "    precision = hits/total_pred\n",
    "    return recall, precision\n",
    "def get_topn(r_pred, train_mat, n=10):\n",
    "    unrated_items = r_pred * (train_mat==0)\n",
    "    idx = np.argsort(-unrated_items)\n",
    "    return idx[:,:n]\n",
    "topn = get_topn(r_pred=r_pred, train_mat=train_mat, n=5)\n",
    "topncos = get_topn(r_pred=r_pred_cos, train_mat=train_mat, n=5)\n",
    "recall, precision = recall_precision(topn=topn, test_mat=test_mat)\n",
    "print('Pearson recall:%.4f; precision:%.4f'%(recall,precision)) \n",
    "recall, precision = recall_precision(topn=topn, test_mat=test_mat)\n",
    "print('Cosine recall:%.4f; precision:%.4f'%(recall,precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8691e46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013623504950467507"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg = calculate_ndcg_for_arrays(topn, test_mat)\n",
    "ndcg_=np.mean(ndcg)\n",
    "ndcg_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279460e1",
   "metadata": {},
   "source": [
    "### BPR-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beebf21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(object):\n",
    "\n",
    "    def __init__(self, numUsers, numItems, lamI = 6e-2, lamJ = 6e-3, learningRate = 0.1):\n",
    "        self._numUsers = numUsers\n",
    "        self._numItems = numItems\n",
    "        self._lamI = lamI\n",
    "        self._lamJ = lamJ\n",
    "        self._learningRate = learningRate\n",
    "        self._users = set()\n",
    "        self._items = set()\n",
    "        self._Iu = defaultdict(set)\n",
    "        \n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def train(self, trainData, epochs=30, batchSize=500):\n",
    "        \n",
    "        # correlation matrix\n",
    "        self.C =np.random.rand(self._numItems, self._numItems)  \n",
    "        for l in range(self._numItems):\n",
    "            self.C[l][l] = 0\n",
    "            for n in range(l, self._numItems):\n",
    "                self.C[l][n] = self.C[n][l]\n",
    "              \n",
    "        # change batch_size to min(batch-size, len(train))\n",
    "        if len(trainData) < batchSize:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(trainData)))\n",
    "            batchSize = len(trainData)\n",
    "                  \n",
    "        self._trainDict, self._users, self._items = self._dataPretreatment(trainData)\n",
    "        N = len(trainData) * epochs\n",
    "        users, pItems, nItems = self._sampling(N)\n",
    "        itr = 0\n",
    "        t2 = t0 = time.time()\n",
    "        while (itr+1)*batchSize < N:\n",
    "      \n",
    "            self._mbgd(\n",
    "                users[itr*batchSize: (itr+1)*batchSize],\n",
    "                pItems[itr*batchSize: (itr+1)*batchSize],\n",
    "                nItems[itr*batchSize: (itr+1)*batchSize]\n",
    "            )\n",
    "            \n",
    "            itr += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.3f%% ) in %.1f seconds\" %(str(itr*batchSize), 100.0 * float(itr*batchSize)/N, t2 - t0))\n",
    "            sys.stderr.flush()\n",
    "        if N > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %.2f samples per second\\n\" % (t2 - t0, N*1.0/(t2 - t0)))\n",
    "            sys.stderr.flush()\n",
    "            \n",
    "            \n",
    "    def _mbgd(self, users, pItems, nItems):\n",
    "        \n",
    "        prev = -2**10\n",
    "        for _ in range(30):\n",
    "            \n",
    "            gradientC = defaultdict(float)\n",
    "            obj = 0\n",
    "\n",
    "            for ind in range(len(users)):\n",
    "                u, i, j = users[ind], pItems[ind], nItems[ind]\n",
    "                x_ui = sum([self.C[i][l] for l in self._Iu[u] if i != l])\n",
    "                x_uj = sum([self.C[j][l] for l in self._Iu[u]])\n",
    "                x_uij =  x_ui - x_uj\n",
    "                \n",
    "                for l in self._Iu[u]:\n",
    "                    if l != i:\n",
    "                        gradientC[(i,l)] += (1-self.sigmoid(x_uij)) + self._lamI * self.C[i][l]**2\n",
    "                        gradientC[(l,i)] += (1-self.sigmoid(x_uij)) + self._lamI * self.C[l][i]**2\n",
    "                    gradientC[(j,l)] += -(1-self.sigmoid(x_uij)) + self._lamJ * self.C[j][l]**2\n",
    "                    gradientC[(l,j)] += -(1-self.sigmoid(x_uij)) + self._lamJ * self.C[l][j]**2\n",
    "                    \n",
    "                    obj -= 2*self._lamI * self.C[i][l]**2 + 2*self._lamJ * self.C[j][l]**2\n",
    "                    \n",
    "                obj += log(self.sigmoid(x_uij))\n",
    "            \n",
    "            #print 'OBJ: ', obj\n",
    "            if prev > obj: \n",
    "                break\n",
    "            prev = obj\n",
    "            \n",
    "            for a,b in gradientC:\n",
    "                self.C[a][b] += self._learningRate * gradientC[(a,b)]\n",
    "            \n",
    "        #print _, '\\n'\n",
    "        \n",
    "    def _sampling(self, N):\n",
    "        \n",
    "        sys.stderr.write(\"Generating %s random training samples\\n\" % str(N))\n",
    "        userList = list(self._users)\n",
    "        userIndex = np.random.randint(0, len(self._users), N)\n",
    "        pItems, nItems = [], []\n",
    "        cnt = 0\n",
    "        for index in userIndex:\n",
    "            u = userList[index]\n",
    "            i = self._trainDict[u][np.random.randint(len(self._Iu[u]))]\n",
    "            pItems.append(i)\n",
    "            j = np.random.randint(self._numItems)\n",
    "            while j in self._Iu[u]:\n",
    "                j = np.random.randint(self._numItems)\n",
    "            nItems.append(j)\n",
    "            \n",
    "            cnt += 1\n",
    "            if not cnt %10000:\n",
    "                sys.stderr.write(\"\\rGenerated %s\" %(str(cnt)))\n",
    "                sys.stderr.flush()\n",
    "        return userIndex, pItems, nItems\n",
    "\n",
    "    def predictionsKNN(self, K, u):\n",
    "        #slow\n",
    "        if K >= len(self._Iu[u]):\n",
    "            res = np.sum([self.C[:,l] for l in self._Iu[u]], 0)\n",
    "        else:\n",
    "            res = []\n",
    "            for i in range(self._numItems):\n",
    "                res.append(sum(sorted([self.C[i][l] for l in self._Iu[u]], reverse=True)[:K]))\n",
    "        return res\n",
    "\n",
    "    def predictionsAll(self, u):\n",
    "        \n",
    "        res = np.sum([self.C[:,l] for l in self._Iu[u]], 0)\n",
    "        return res\n",
    "\n",
    "    def prediction(self, u, i):\n",
    "        \n",
    "        scores = self.predictionsAll(u)\n",
    "        return scores[i] > sorted(scores)[self._numItem*0.8]\n",
    "\n",
    "    def _dataPretreatment(self, data):\n",
    "        dataDict = defaultdict(list)\n",
    "        items = set()\n",
    "        for u, i in data:\n",
    "            self._Iu[u].add(i)\n",
    "            dataDict[u].append(i)\n",
    "            items.add(i)\n",
    "        return dataDict, set(dataDict.keys()), items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11d1e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=[]\n",
    "prec=[]\n",
    "recall=[]\n",
    "ndcg=[]\n",
    "\n",
    "class BPR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPR, self).__init__()\n",
    "        self.W = None             # user matrix\n",
    "        self.H = None             # item matrix\n",
    "        \n",
    "        self.uid = None\n",
    "        self.iid = None\n",
    "        \n",
    "        # 用户u对应他访问过的所有items集合\n",
    "        self.train_user_items = None\n",
    "        self.test_user_items = None\n",
    "        \n",
    "        # (u, i, rating) dataset\n",
    "        self.train = None\n",
    "        self.test = None\n",
    "        \n",
    "    def _split(self, df, ratio):\n",
    "        train = pd.DataFrame(columns = df.columns, dtype=int)\n",
    "        test = pd.DataFrame(columns = df.columns, dtype=int)\n",
    "        for i in self.uid:\n",
    "            train_1, test_1 = train_test_split(df[df.iloc[:, 0] == i], train_size = ratio, shuffle = True, random_state = 5)\n",
    "            train = pd.concat([train, train_1])\n",
    "            test = pd.concat([test, test_1])\n",
    "        return train, test    \n",
    "    \n",
    "    def preprocess(self, df, train_size=0.8):\n",
    "        df = df.rename(columns = {df.columns[0]: 'ori_uid', df.columns[1]: 'ori_iid', df.columns[2]: 'rating'})\n",
    "        df = df.groupby('ori_uid').filter(lambda x: x['ori_uid'].count()>=10)\n",
    "        uid_map = pd.DataFrame({\"ori_uid\": np.asarray(list(set(df.iloc[:,0].values)))})\n",
    "        uid_map[\"serial_uid\"] = uid_map.index\n",
    "        iid_map = pd.DataFrame({\"ori_iid\": np.asarray(list(set(df.iloc[:,1].values)))})\n",
    "        iid_map[\"serial_iid\"] = iid_map.index\n",
    "        \n",
    "        self.uid = uid_map[\"serial_uid\"].values\n",
    "        self.iid = iid_map[\"serial_iid\"].values\n",
    "        \n",
    "        df = df.merge(uid_map, left_on = 'ori_uid', right_on = 'ori_uid', how=\"left\")\n",
    "        df = df.merge(iid_map, left_on = 'ori_iid', right_on = 'ori_iid', how=\"left\")\n",
    "        df = df[['serial_uid', 'serial_iid', 'rating']]\n",
    "        \n",
    "        train, test = self._split(df, train_size)\n",
    "        \n",
    "        self.train_user_items = train.groupby(train.columns[0])[train.columns[1]].apply(lambda x: list(x)).to_list()\n",
    "        self.test_user_items = test.groupby(test.columns[0])[test.columns[1]].apply(lambda x: list(x)).to_list()\n",
    "        \n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        return self.train_user_items, self.test_user_items, self.iid, self.uid\n",
    "    \n",
    "    def generate_train_batch(self, batch, sets,ite=10):\n",
    "        train = []\n",
    "        for b in range(batch):\n",
    "            u = self.uid[np.random.randint(0, len(self.uid))]\n",
    "            i = sets[u][np.random.randint(0, len(sets[u]))]\n",
    "            lis = [u,i]\n",
    "            for a in range(ite):\n",
    "                j = self.iid[np.random.randint(0, len(self.iid))]\n",
    "                while j in sets[u]:\n",
    "                    j = self.iid[np.random.randint(0, len(self.iid))]\n",
    "                lis.append(j)\n",
    "            train.append(lis)\n",
    "        return np.asarray(train) \n",
    "    \n",
    "    def fit(self, k, stepsize=0.05, max_iter=10, batch=10000, n=10,ite=10):\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        \n",
    "        self.W = nn.Parameter(torch.rand(len(self.uid), k).to(device) * 0.01)    # 初始化 W，H\n",
    "        self.H = nn.Parameter(torch.rand(len(self.iid), k).to(device) * 0.01)  \n",
    "        \n",
    "        # 创建字典：用户u对应他访问过的所有items集合\n",
    "#         self.train_user_items = df.groupby(df.columns[0])[df.columns[1]].apply(lambda x: np.array(x)).to_list()\n",
    "        \n",
    "        optimizer = optim.Adam([self.W, self.H], lr=stepsize)     # 主模型优化器\n",
    "        for x in range(max_iter):\n",
    "            #取训练批次：uij三元组\n",
    "            uij = self.generate_train_batch(batch, self.train_user_items,ite)\n",
    "            \n",
    "            u = uij[:, 0]\n",
    "            i = uij[:, 1]\n",
    "            \n",
    "            u_emb = self.W[u]\n",
    "            i_emb = self.H[i]\n",
    "            \n",
    "            \n",
    "            j_embs = torch.zeros_like(i_emb) \n",
    "            \n",
    "            for i in range(2, 2 + ite):  \n",
    "                j = uij[:, i]  \n",
    "                j_emb = self.H[j]  \n",
    "                j_embs+=torch.exp(u_emb*j_emb)\n",
    "            j_embs=j_embs/ite\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = torch.sum((torch.sum(-u_emb * i_emb +torch.log(torch.exp(u_emb * i_emb) +j_embs+1e-8))+1e-8))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f\"Train | {x+1}/{max_iter}, BPR loss: {loss.item() / batch}\")\n",
    "            \n",
    "            rec, pre, ndcg = self.performance(n)\n",
    "            print(f'Valid | {x+1}/{max_iter}, Pre@{n}: {pre}, Recl@{n}: {rec}, NDCG@{n}: {ndcg}')\n",
    "            print('---------------------------------------------------')\n",
    "    \n",
    "    def _predict(self, uid, items, n):\n",
    "        scores = torch.mv(self.H[items], self.W[uid])\n",
    "        if n > scores.shape[0]: \n",
    "            n = scores.shape[0]\n",
    "        top_N_val, top_N_idx = torch.topk(scores, k=n)\n",
    "        return list(zip(items[top_N_idx.cpu()], top_N_val.cpu()))\n",
    "\n",
    "    def NDCG(self, uid, n):         # 用模型排序+真实分数计算 DCG, 重排后计算 iDCG\n",
    "        test_user = self.test[self.test.iloc[:, 0] == uid]\n",
    "        rating = self._predict(uid, test_user.iloc[:, 1].values, n)\n",
    "        irating =sorted(test_user.iloc[:, 2].values, reverse=True)\n",
    "        dcg = 0\n",
    "        idcg = 0\n",
    "        if n > len(irating): n = len(irating)  \n",
    "        for i in range(n):\n",
    "            r = test_user[test_user.iloc[:, 1]==rating[i][0]].iloc[0, 2]\n",
    "            dcg += 1.0 * (2**r - 1) / math.log(i + 2, 2)\n",
    "            idcg += 1.0 * (2**irating[i] - 1) / math.log(i + 2, 2)\n",
    "        return dcg / idcg\n",
    "\n",
    "    def performance(self, n):      # Output recall@n, precision@n, NDCG@n\n",
    "        hit = 0\n",
    "        n_recall = 0\n",
    "        n_precision = 0\n",
    "        ndcg = 0\n",
    "        \n",
    "        for i in self.uid:\n",
    "            # Items that User i hasn't tried in training set\n",
    "            unknown_items = np.setdiff1d(self.iid, self.train_user_items[i])\n",
    "            # Items that User i actually tried in testing set\n",
    "            known_items = self.test_user_items[i]\n",
    "            \n",
    "            #目标：预测 unknown items 中的top_N，若击中test中的items，则为有效预测\n",
    "            ru = self._predict(i, unknown_items, n)\n",
    "            \n",
    "            hit += sum(1 for item, pui in ru if item in known_items)\n",
    "            n_recall += len(known_items)\n",
    "            n_precision += n\n",
    "            ndcg += self.NDCG(i, n)\n",
    "            \n",
    "        recall = hit / (1.0 * n_recall)\n",
    "        precision = hit / (1.0 * n_precision)\n",
    "        ndcg /= len(self.uid)\n",
    "        return recall, precision, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002a900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./ml-1m/ratings.dat\", sep=\"::\", names=['user id', 'item id', 'rating', 'timestamp'], engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7abbb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = BPR()\n",
    "train_user_items, test_user_items,iid, uid=model1.preprocess(df1)\n",
    "train = model1.train\n",
    "test = model1.test\n",
    "train1 = [tuple(i) for i in train.iloc[:, :2].values.tolist()]\n",
    "test1 = [tuple(i) for i in test.iloc[:, :2].values.tolist()]\n",
    "u = len(model1.uid)\n",
    "i = len(model1.iid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def963df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 15955160 random training samples\n",
      "Processed 100000 ( 0.627% ) in 296.0 seconds"
     ]
    }
   ],
   "source": [
    "AKNN=KNN(u,i)\n",
    "AKNN.train(train1,20,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aknn=[]\n",
    "for i in uid:\n",
    "    aknn.append(AKNN.predictionsKNN(5, i))\n",
    "aknnt= torch.tensor(aknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6285a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def NDCG(uid, n):         # 用模型排序+真实分数计算 DCG, 重排后计算 iDCG\n",
    "    test_user = test[test.iloc[:, 0] == uid]\n",
    "    rating = predict(uid, test_user.iloc[:, 1].values, n)\n",
    "    irating =sorted(test_user.iloc[:, 2].values, reverse=True)\n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "    if n > len(irating): n = len(irating)  \n",
    "    for i in range(n):\n",
    "        r = test_user[test_user.iloc[:, 1]==rating[i][0]].iloc[0, 2]\n",
    "        dcg += 1.0 * (2**r - 1) / math.log(i + 2, 2)\n",
    "        idcg += 1.0 * (2**irating[i] - 1) / math.log(i + 2, 2)\n",
    "    return dcg / idcg\n",
    "\n",
    "def predict( uid, items, n):\n",
    "    scores =aknnt[uid][items]\n",
    "    if n > scores.shape[0]: \n",
    "        n = scores.shape[0]\n",
    "    top_N_val, top_N_idx = torch.topk(scores, k=n)\n",
    "    return list(zip(items[top_N_idx.cpu()], top_N_val.cpu())) \n",
    "\n",
    "def performanceout(n):\n",
    "    hit = 0\n",
    "    n_recall = 0\n",
    "    n_precision = 0\n",
    "    ndcg = 0\n",
    "    for i in uid:\n",
    "        unknown_items = np.setdiff1d(iid, train_user_items[i])\n",
    "\n",
    "        known_items = test_user_items[i]\n",
    "\n",
    "        ru = predict(i, unknown_items, n)\n",
    "        \n",
    "        hit += sum(1 for item, pui in ru if item in known_items)\n",
    "        n_recall += len(known_items)\n",
    "        n_precision += n\n",
    "        ndcg += NDCG(i, n)\n",
    "\n",
    "    recall = hit / (1.0 * n_recall)\n",
    "    precision = hit / (1.0 * n_precision)\n",
    "    ndcg /= len(uid)\n",
    "    return recall, precision, ndcg    \n",
    "    \n",
    "performanceout(10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457bed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec, pre, ndcg = performanceout(10) \n",
    "print(f' Pre@10: {pre}, Rec@10: {rec}, NDCG@10: {ndcg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b86f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
