{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f0900197",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "f0900197",
        "outputId": "4dab05f7-119b-479c-9e37-ea1cf1fc7a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/SDSC3002_Project'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Colab setting\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/SDSC3002_Project/')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ce33bdf0",
      "metadata": {
        "id": "ce33bdf0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "c666ec5c",
      "metadata": {
        "id": "c666ec5c"
      },
      "outputs": [],
      "source": [
        "class BPR(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BPR, self).__init__()\n",
        "        self.W = None             # user matrix\n",
        "        self.H = None             # item matrix\n",
        "\n",
        "        self.uid = None\n",
        "        self.iid = None\n",
        "\n",
        "        # 用户u对应他访问过的所有items集合\n",
        "        self.train_user_items = None\n",
        "        self.test_user_items = None\n",
        "\n",
        "        # (u, i, rating) dataset\n",
        "        self.train = None\n",
        "        self.test = None\n",
        "\n",
        "    def _split(self, df, ratio):\n",
        "        train = pd.DataFrame(columns = df.columns, dtype=int)\n",
        "        test = pd.DataFrame(columns = df.columns, dtype=int)\n",
        "        for i in self.uid:\n",
        "            train_1, test_1 = train_test_split(df[df.iloc[:, 0] == i], train_size = ratio, shuffle = True, random_state = 5)\n",
        "            train = pd.concat([train, train_1])\n",
        "            test = pd.concat([test, test_1])\n",
        "        return train, test\n",
        "\n",
        "    def preprocess(self, df, train_size=0.8):\n",
        "        df = df.rename(columns = {df.columns[0]: 'ori_uid', df.columns[1]: 'ori_iid', df.columns[2]: 'rating'})\n",
        "        df = df.groupby('ori_uid').filter(lambda x: x['ori_uid'].count()>=10)\n",
        "        uid_map = pd.DataFrame({\"ori_uid\": np.asarray(list(set(df.iloc[:,0].values)))})\n",
        "        uid_map[\"serial_uid\"] = uid_map.index\n",
        "        iid_map = pd.DataFrame({\"ori_iid\": np.asarray(list(set(df.iloc[:,1].values)))})\n",
        "        iid_map[\"serial_iid\"] = iid_map.index\n",
        "\n",
        "        self.uid = uid_map[\"serial_uid\"].values\n",
        "        self.iid = iid_map[\"serial_iid\"].values\n",
        "\n",
        "        df = df.merge(uid_map, left_on = 'ori_uid', right_on = 'ori_uid', how=\"left\")\n",
        "        df = df.merge(iid_map, left_on = 'ori_iid', right_on = 'ori_iid', how=\"left\")\n",
        "        df = df[['serial_uid', 'serial_iid', 'rating']]\n",
        "\n",
        "        train, test = self._split(df, train_size)\n",
        "\n",
        "        self.train_user_items = train.groupby(train.columns[0])[train.columns[1]].apply(lambda x: list(x)).to_list()\n",
        "        self.test_user_items = test.groupby(test.columns[0])[test.columns[1]].apply(lambda x: list(x)).to_list()\n",
        "\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "\n",
        "        count = self.train.groupby(['serial_uid', 'serial_iid']).size().reset_index(name='count')\n",
        "        self.train = pd.merge(self.train.copy(), count, on=['serial_uid', 'serial_iid'], how='inner')\n",
        "\n",
        "\n",
        "    def generate_train_batch(self, batch, sets,m):\n",
        "        train_u = []\n",
        "        train_i = []\n",
        "        train_js = []\n",
        "        for b in range(batch):\n",
        "            u = self.uid[np.random.randint(0, len(self.uid))]\n",
        "            i = sets[u][np.random.randint(0, len(sets[u]))]\n",
        "            js = random.sample(list(set(self.iid) - set(sets[u])), m)\n",
        "\n",
        "            train_u.append(u)\n",
        "            train_i.append(i)\n",
        "            train_js.append(js)\n",
        "        return np.array(train_u), np.array(train_i), np.array(train_js)\n",
        "\n",
        "    def fit(self, k, m=10, stepsize=0.05, max_iter=10, batch=10000, n=10):\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(device)\n",
        "\n",
        "        self.W = nn.Parameter(torch.rand(len(self.uid), k).to(device) * 0.01)    # 初始化 W，H\n",
        "        self.H = nn.Parameter(torch.rand(len(self.iid), k).to(device) * 0.01)\n",
        "\n",
        "        loss = 0\n",
        "        criterion = torch.nn.BCELoss()\n",
        "        optimizer = optim.Adam([self.W, self.H], lr=stepsize)     # 主模型优化器\n",
        "        for x in range(max_iter):\n",
        "            #取训练批次：uij三元组\n",
        "            u, i, js = self.generate_train_batch(batch, self.train_user_items, m)\n",
        "            # u = uij[:, 0].astype(int)\n",
        "            # i = uij[:, 1].astype(int)\n",
        "            # js = uij[:, 2]\n",
        "\n",
        "            u_emb = self.W[u]\n",
        "            i_emb = self.H[i]\n",
        "\n",
        "            # TODO: optimize positve\n",
        "            optimizer.zero_grad()\n",
        "            pos_pred = torch.sigmoid(torch.sum(u_emb * i_emb,dim = 1))\n",
        "#             pos_pred = (pos_pred >= 0.5).to(torch.int64)\n",
        "\n",
        "            bceloss_pos = criterion(pos_pred, torch.FloatTensor([1]*batch).to(device))\n",
        "            bceloss_pos.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "\n",
        "            loss += bceloss_pos\n",
        "\n",
        "            # TODO: optimize negative\n",
        "\n",
        "            for i in range(m):\n",
        "                optimizer.zero_grad()\n",
        "                j_emb = self.H[js[:, i]]\n",
        "                neg_pred = torch.sigmoid(torch.sum(u_emb * j_emb,dim = 1))\n",
        "                # neg_pred = (neg_pred >= 0.5).to(torch.int64)\n",
        "                bceloss_neg = criterion(neg_pred, torch.FloatTensor([0]*batch).to(device))\n",
        "                bceloss_neg.backward(retain_graph=True)\n",
        "                optimizer.step()\n",
        "\n",
        "                loss += bceloss_neg\n",
        "\n",
        "            loss = loss/(m+1)\n",
        "\n",
        "            print(f\"Train | {x+1}/{max_iter}, BPR loss: {loss / batch}\")\n",
        "            with torch.no_grad():\n",
        "                rec, pre, ndcg = self.performance(n)\n",
        "            print(f'Valid | {x+1}/{max_iter}, Pre@{n}: {pre}, Rec@{n}: {rec}, NDCG@{n}: {ndcg}')\n",
        "            print('---------------------------------------------------')\n",
        "\n",
        "    def _predict(self, uid, items, n):\n",
        "\n",
        "        scores = torch.mv(self.H[items], self.W[uid])\n",
        "        if n > scores.shape[0]:\n",
        "            n = scores.shape[0]\n",
        "        top_N_val, top_N_idx = torch.topk(scores, k=n)\n",
        "        return list(zip(items[top_N_idx.cpu()], top_N_val.cpu()))\n",
        "\n",
        "    def NDCG(self, uid, n):         # 用模型排序+真实分数计算 DCG, 重排后计算 iDCG\n",
        "        # test 集中，uid 评过的 items\n",
        "        test_user = self.test[self.test.iloc[:, 0] == uid]\n",
        "\n",
        "        # 对这些 items 做 top-k\n",
        "        rating = self._predict(uid, test_user.iloc[:, 1].values, n)\n",
        "\n",
        "        # 排序真实评分\n",
        "        irating =sorted(test_user.iloc[:, 2].values, reverse=True)\n",
        "        irating = np.asarray(irating)\n",
        "\n",
        "        if n > len(irating): n = len(irating)\n",
        "\n",
        "        # 取出模型排序下 merge 到的真实分数\n",
        "        rating_df = pd.DataFrame(rating, columns=['serial_iid', 'pred_rating'])\n",
        "        merged_df = pd.merge(rating_df, test_user, on='serial_iid')\n",
        "        r = np.array(merged_df['rating'])\n",
        "\n",
        "        # 求 log 分母\n",
        "        log = np.log(np.arange(2, n + 2))\n",
        "\n",
        "        # 求 dcg 和 idcg\n",
        "        dcg = np.log(2) * np.sum((2**r[:n] - 1) / log)\n",
        "        idcg = np.log(2) * np.sum((2**irating[:n] - 1) / log)\n",
        "\n",
        "        return dcg / idcg\n",
        "\n",
        "    def performance(self, n):      # Output recall@n, precision@n, NDCG@n\n",
        "        hit = 0\n",
        "        n_recall = 0\n",
        "        n_precision = 0\n",
        "        ndcg = 0\n",
        "\n",
        "        for i in self.uid:\n",
        "            # Items that User i hasn't tried in training set\n",
        "            unknown_items = np.setdiff1d(self.iid, self.train_user_items[i])\n",
        "            # Items that User i actually tried in testing set\n",
        "            known_items = self.test_user_items[i]\n",
        "\n",
        "            #目标：预测 unknown items 中的top_N，若击中test中的items，则为有效预测\n",
        "            ru = self._predict(i, unknown_items, n)\n",
        "\n",
        "            hit += sum(1 for item, pui in ru if item in known_items)\n",
        "            n_recall += len(known_items)\n",
        "            n_precision += n\n",
        "            ndcg += self.NDCG(i, n)\n",
        "\n",
        "        recall = hit / (1.0 * n_recall)\n",
        "        precision = hit / (1.0 * n_precision)\n",
        "        ndcg /= len(self.uid)\n",
        "        return recall, precision, ndcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "319ffcc3",
      "metadata": {
        "id": "319ffcc3"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(\"./ml-100k/u.data\", sep=\"\\t\", names=['user id', 'item id', 'rating', 'timestamp'])\n",
        "df2 = pd.read_csv(\"./ml-1m/ratings.dat\", sep=\"::\", names=['user id', 'item id', 'rating', 'timestamp'], engine='python')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a2cdfb1",
      "metadata": {
        "id": "0a2cdfb1"
      },
      "source": [
        "### 100K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "da1781ca",
      "metadata": {
        "scrolled": true,
        "id": "da1781ca"
      },
      "outputs": [],
      "source": [
        "model1 = BPR()\n",
        "model1.preprocess(df1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "26dbdb06",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "26dbdb06",
        "outputId": "22c26a49-140a-4077-dc2f-21e0e84c1543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train | 1/100, BPR loss: 6.93640613462776e-05\n",
            "Valid | 1/100, Pre@10: 0.1151643690349947, Rec@10: 0.05328492223149011, NDCG@10: 0.7244074890664074\n",
            "---------------------------------------------------\n",
            "Train | 2/100, BPR loss: 7.263950101332739e-05\n",
            "Valid | 2/100, Pre@10: 0.13149522799575822, Rec@10: 0.060840979343506206, NDCG@10: 0.7302770170592982\n",
            "---------------------------------------------------\n",
            "Train | 3/100, BPR loss: 7.277988333953544e-05\n",
            "Valid | 3/100, Pre@10: 0.11060445387062566, Rec@10: 0.05117511407683627, NDCG@10: 0.7206196409033162\n",
            "---------------------------------------------------\n",
            "Train | 4/100, BPR loss: 7.278125849552453e-05\n",
            "Valid | 4/100, Pre@10: 0.07783669141039236, Rec@10: 0.0360139345468819, NDCG@10: 0.7096642750416402\n",
            "---------------------------------------------------\n",
            "Train | 5/100, BPR loss: 7.278017437784001e-05\n",
            "Valid | 5/100, Pre@10: 0.0799575821845175, Rec@10: 0.03699524066532555, NDCG@10: 0.7093194396639823\n",
            "---------------------------------------------------\n",
            "Train | 6/100, BPR loss: 7.277962140506133e-05\n",
            "Valid | 6/100, Pre@10: 0.10816542948038176, Rec@10: 0.050046612040626076, NDCG@10: 0.721135174591164\n",
            "---------------------------------------------------\n",
            "Train | 7/100, BPR loss: 7.277829718077555e-05\n",
            "Valid | 7/100, Pre@10: 0.13923647932131494, Rec@10: 0.06442274667582552, NDCG@10: 0.7345431070019778\n",
            "---------------------------------------------------\n",
            "Train | 8/100, BPR loss: 7.27743681636639e-05\n",
            "Valid | 8/100, Pre@10: 0.1565217391304348, Rec@10: 0.07242039154114126, NDCG@10: 0.7377836933990993\n",
            "---------------------------------------------------\n",
            "Train | 9/100, BPR loss: 7.276315591298044e-05\n",
            "Valid | 9/100, Pre@10: 0.1623541887592789, Rec@10: 0.07511898336686129, NDCG@10: 0.7402089460066357\n",
            "---------------------------------------------------\n",
            "Train | 10/100, BPR loss: 7.273265509866178e-05\n",
            "Valid | 10/100, Pre@10: 0.16468716861081653, Rec@10: 0.0761984200971493, NDCG@10: 0.7409707160406839\n",
            "---------------------------------------------------\n",
            "Train | 11/100, BPR loss: 7.266026659635827e-05\n",
            "Valid | 11/100, Pre@10: 0.1653234358430541, Rec@10: 0.0764928119326824, NDCG@10: 0.7401347722511307\n",
            "---------------------------------------------------\n",
            "Train | 12/100, BPR loss: 7.25198769941926e-05\n",
            "Valid | 12/100, Pre@10: 0.16659597030752915, Rec@10: 0.0770815956037486, NDCG@10: 0.7411477525449569\n",
            "---------------------------------------------------\n",
            "Train | 13/100, BPR loss: 7.22901095286943e-05\n",
            "Valid | 13/100, Pre@10: 0.16712619300106044, Rec@10: 0.0773269221333595, NDCG@10: 0.7417771827787982\n",
            "---------------------------------------------------\n",
            "Train | 14/100, BPR loss: 7.196500519057736e-05\n",
            "Valid | 14/100, Pre@10: 0.16733828207847296, Rec@10: 0.07742505274520387, NDCG@10: 0.7416780493107986\n",
            "---------------------------------------------------\n",
            "Train | 15/100, BPR loss: 7.155501953093335e-05\n",
            "Valid | 15/100, Pre@10: 0.16659597030752915, Rec@10: 0.0770815956037486, NDCG@10: 0.7412558255739986\n",
            "---------------------------------------------------\n",
            "Train | 16/100, BPR loss: 7.106543489499018e-05\n",
            "Valid | 16/100, Pre@10: 0.16744432661717923, Rec@10: 0.07747411805112604, NDCG@10: 0.741885509905225\n",
            "---------------------------------------------------\n",
            "Train | 17/100, BPR loss: 7.050946442177519e-05\n",
            "Valid | 17/100, Pre@10: 0.1672322375397667, Rec@10: 0.07737598743928169, NDCG@10: 0.741210232878203\n",
            "---------------------------------------------------\n",
            "Train | 18/100, BPR loss: 6.989582470851019e-05\n",
            "Valid | 18/100, Pre@10: 0.16786850477200424, Rec@10: 0.07767037927481478, NDCG@10: 0.7413418977697774\n",
            "---------------------------------------------------\n",
            "Train | 19/100, BPR loss: 6.923350156284869e-05\n",
            "Valid | 19/100, Pre@10: 0.16861081654294804, Rec@10: 0.07801383641627005, NDCG@10: 0.7416577529425163\n",
            "---------------------------------------------------\n",
            "Train | 20/100, BPR loss: 6.852616934338585e-05\n",
            "Valid | 20/100, Pre@10: 0.16882290562036056, Rec@10: 0.07811196702811442, NDCG@10: 0.7414188928398405\n",
            "---------------------------------------------------\n",
            "Train | 21/100, BPR loss: 6.779020623071119e-05\n",
            "Valid | 21/100, Pre@10: 0.16914103923647933, Rec@10: 0.07825916294588096, NDCG@10: 0.7417761164819984\n",
            "---------------------------------------------------\n",
            "Train | 22/100, BPR loss: 6.70171866659075e-05\n",
            "Valid | 22/100, Pre@10: 0.16670201484623542, Rec@10: 0.07713066090967077, NDCG@10: 0.7419795061814105\n",
            "---------------------------------------------------\n",
            "Train | 23/100, BPR loss: 6.61965023027733e-05\n",
            "Valid | 23/100, Pre@10: 0.16733828207847296, Rec@10: 0.07742505274520387, NDCG@10: 0.7415259035269315\n",
            "---------------------------------------------------\n",
            "Train | 24/100, BPR loss: 6.536687578773126e-05\n",
            "Valid | 24/100, Pre@10: 0.16659597030752915, Rec@10: 0.0770815956037486, NDCG@10: 0.7422625728053567\n",
            "---------------------------------------------------\n",
            "Train | 25/100, BPR loss: 6.451478111557662e-05\n",
            "Valid | 25/100, Pre@10: 0.16606574761399787, Rec@10: 0.07683626907413768, NDCG@10: 0.7419580324688139\n",
            "---------------------------------------------------\n",
            "Train | 26/100, BPR loss: 6.363967258948833e-05\n",
            "Valid | 26/100, Pre@10: 0.16712619300106044, Rec@10: 0.0773269221333595, NDCG@10: 0.7418630947169226\n",
            "---------------------------------------------------\n",
            "Train | 27/100, BPR loss: 6.274842598941177e-05\n",
            "Valid | 27/100, Pre@10: 0.16744432661717923, Rec@10: 0.07747411805112604, NDCG@10: 0.7418584871843147\n",
            "---------------------------------------------------\n",
            "Train | 28/100, BPR loss: 6.185313395690173e-05\n",
            "Valid | 28/100, Pre@10: 0.167762460233298, Rec@10: 0.0776213139688926, NDCG@10: 0.7418841846043905\n",
            "---------------------------------------------------\n",
            "Train | 29/100, BPR loss: 6.092722833273001e-05\n",
            "Valid | 29/100, Pre@10: 0.16765641569459172, Rec@10: 0.07757224866297041, NDCG@10: 0.7418409061413856\n",
            "---------------------------------------------------\n",
            "Train | 30/100, BPR loss: 5.999219138175249e-05\n",
            "Valid | 30/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7416748486316811\n",
            "---------------------------------------------------\n",
            "Train | 31/100, BPR loss: 5.905671059736051e-05\n",
            "Valid | 31/100, Pre@10: 0.16765641569459172, Rec@10: 0.07757224866297041, NDCG@10: 0.741098598403117\n",
            "---------------------------------------------------\n",
            "Train | 32/100, BPR loss: 5.81327076361049e-05\n",
            "Valid | 32/100, Pre@10: 0.16861081654294804, Rec@10: 0.07801383641627005, NDCG@10: 0.7415558546670336\n",
            "---------------------------------------------------\n",
            "Train | 33/100, BPR loss: 5.7185876357834786e-05\n",
            "Valid | 33/100, Pre@10: 0.16839872746553552, Rec@10: 0.0779157058044257, NDCG@10: 0.7415019898535657\n",
            "---------------------------------------------------\n",
            "Train | 34/100, BPR loss: 5.624135519610718e-05\n",
            "Valid | 34/100, Pre@10: 0.167762460233298, Rec@10: 0.0776213139688926, NDCG@10: 0.7412043878879573\n",
            "---------------------------------------------------\n",
            "Train | 35/100, BPR loss: 5.529107147594914e-05\n",
            "Valid | 35/100, Pre@10: 0.168186638388123, Rec@10: 0.07781757519258133, NDCG@10: 0.7414619585486273\n",
            "---------------------------------------------------\n",
            "Train | 36/100, BPR loss: 5.436121136881411e-05\n",
            "Valid | 36/100, Pre@10: 0.16765641569459172, Rec@10: 0.07757224866297041, NDCG@10: 0.741562718644258\n",
            "---------------------------------------------------\n",
            "Train | 37/100, BPR loss: 5.3417676099343225e-05\n",
            "Valid | 37/100, Pre@10: 0.16808059384941676, Rec@10: 0.07776850988665915, NDCG@10: 0.7414230770672671\n",
            "---------------------------------------------------\n",
            "Train | 38/100, BPR loss: 5.249432797427289e-05\n",
            "Valid | 38/100, Pre@10: 0.167762460233298, Rec@10: 0.0776213139688926, NDCG@10: 0.741375848597646\n",
            "---------------------------------------------------\n",
            "Train | 39/100, BPR loss: 5.1531118515413254e-05\n",
            "Valid | 39/100, Pre@10: 0.16797454931071049, Rec@10: 0.07771944458073696, NDCG@10: 0.741799926669541\n",
            "---------------------------------------------------\n",
            "Train | 40/100, BPR loss: 5.0627928430913016e-05\n",
            "Valid | 40/100, Pre@10: 0.16839872746553552, Rec@10: 0.0779157058044257, NDCG@10: 0.7418329458790824\n",
            "---------------------------------------------------\n",
            "Train | 41/100, BPR loss: 4.9718812078936026e-05\n",
            "Valid | 41/100, Pre@10: 0.168186638388123, Rec@10: 0.07781757519258133, NDCG@10: 0.7418935927553709\n",
            "---------------------------------------------------\n",
            "Train | 42/100, BPR loss: 4.880293636233546e-05\n",
            "Valid | 42/100, Pre@10: 0.167762460233298, Rec@10: 0.0776213139688926, NDCG@10: 0.7419230997809704\n",
            "---------------------------------------------------\n",
            "Train | 43/100, BPR loss: 4.78992733405903e-05\n",
            "Valid | 43/100, Pre@10: 0.16691410392364794, Rec@10: 0.07722879152151514, NDCG@10: 0.7418193699046143\n",
            "---------------------------------------------------\n",
            "Train | 44/100, BPR loss: 4.703395825345069e-05\n",
            "Valid | 44/100, Pre@10: 0.16733828207847296, Rec@10: 0.07742505274520387, NDCG@10: 0.7423231685750747\n",
            "---------------------------------------------------\n",
            "Train | 45/100, BPR loss: 4.616775913746096e-05\n",
            "Valid | 45/100, Pre@10: 0.16712619300106044, Rec@10: 0.0773269221333595, NDCG@10: 0.7422210774378121\n",
            "---------------------------------------------------\n",
            "Train | 46/100, BPR loss: 4.532824459602125e-05\n",
            "Valid | 46/100, Pre@10: 0.16765641569459172, Rec@10: 0.07757224866297041, NDCG@10: 0.7419401363269491\n",
            "---------------------------------------------------\n",
            "Train | 47/100, BPR loss: 4.444805745151825e-05\n",
            "Valid | 47/100, Pre@10: 0.1672322375397667, Rec@10: 0.07737598743928169, NDCG@10: 0.7419878068342428\n",
            "---------------------------------------------------\n",
            "Train | 48/100, BPR loss: 4.3637770431814715e-05\n",
            "Valid | 48/100, Pre@10: 0.16765641569459172, Rec@10: 0.07757224866297041, NDCG@10: 0.7418866409459126\n",
            "---------------------------------------------------\n",
            "Train | 49/100, BPR loss: 4.2823572584893554e-05\n",
            "Valid | 49/100, Pre@10: 0.16744432661717923, Rec@10: 0.07747411805112604, NDCG@10: 0.7422416760222257\n",
            "---------------------------------------------------\n",
            "Train | 50/100, BPR loss: 4.2021558328997344e-05\n",
            "Valid | 50/100, Pre@10: 0.16733828207847296, Rec@10: 0.07742505274520387, NDCG@10: 0.742218388545339\n",
            "---------------------------------------------------\n",
            "Train | 51/100, BPR loss: 4.124657789361663e-05\n",
            "Valid | 51/100, Pre@10: 0.16744432661717923, Rec@10: 0.07747411805112604, NDCG@10: 0.7416921291871803\n",
            "---------------------------------------------------\n",
            "Train | 52/100, BPR loss: 4.0440616430714726e-05\n",
            "Valid | 52/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7417761242994815\n",
            "---------------------------------------------------\n",
            "Train | 53/100, BPR loss: 3.972140621044673e-05\n",
            "Valid | 53/100, Pre@10: 0.16765641569459172, Rec@10: 0.07757224866297041, NDCG@10: 0.741855420537885\n",
            "---------------------------------------------------\n",
            "Train | 54/100, BPR loss: 3.901263698935509e-05\n",
            "Valid | 54/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7427280716327123\n",
            "---------------------------------------------------\n",
            "Train | 55/100, BPR loss: 3.828365879599005e-05\n",
            "Valid | 55/100, Pre@10: 0.16744432661717923, Rec@10: 0.07747411805112604, NDCG@10: 0.7426108329692791\n",
            "---------------------------------------------------\n",
            "Train | 56/100, BPR loss: 3.7594854802591726e-05\n",
            "Valid | 56/100, Pre@10: 0.16733828207847296, Rec@10: 0.07742505274520387, NDCG@10: 0.7428371125614227\n",
            "---------------------------------------------------\n",
            "Train | 57/100, BPR loss: 3.6919333069818094e-05\n",
            "Valid | 57/100, Pre@10: 0.16733828207847296, Rec@10: 0.07742505274520387, NDCG@10: 0.7425216653125488\n",
            "---------------------------------------------------\n",
            "Train | 58/100, BPR loss: 3.627001206041314e-05\n",
            "Valid | 58/100, Pre@10: 0.16744432661717923, Rec@10: 0.07747411805112604, NDCG@10: 0.7421511819264202\n",
            "---------------------------------------------------\n",
            "Train | 59/100, BPR loss: 3.560820550774224e-05\n",
            "Valid | 59/100, Pre@10: 0.16765641569459172, Rec@10: 0.07757224866297041, NDCG@10: 0.7417274296136277\n",
            "---------------------------------------------------\n",
            "Train | 60/100, BPR loss: 3.499552985886112e-05\n",
            "Valid | 60/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7419121777325859\n",
            "---------------------------------------------------\n",
            "Train | 61/100, BPR loss: 3.441474837018177e-05\n",
            "Valid | 61/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7414554135133622\n",
            "---------------------------------------------------\n",
            "Train | 62/100, BPR loss: 3.3806663850555196e-05\n",
            "Valid | 62/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7416022194347224\n",
            "---------------------------------------------------\n",
            "Train | 63/100, BPR loss: 3.319342795293778e-05\n",
            "Valid | 63/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7420981361448127\n",
            "---------------------------------------------------\n",
            "Train | 64/100, BPR loss: 3.266044950578362e-05\n",
            "Valid | 64/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7416121158640973\n",
            "---------------------------------------------------\n",
            "Train | 65/100, BPR loss: 3.209303395124152e-05\n",
            "Valid | 65/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7422805901448246\n",
            "---------------------------------------------------\n",
            "Train | 66/100, BPR loss: 3.162629946018569e-05\n",
            "Valid | 66/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7422228568264913\n",
            "---------------------------------------------------\n",
            "Train | 67/100, BPR loss: 3.107553857262246e-05\n",
            "Valid | 67/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7418668068359363\n",
            "---------------------------------------------------\n",
            "Train | 68/100, BPR loss: 3.057338471990079e-05\n",
            "Valid | 68/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7423717587690946\n",
            "---------------------------------------------------\n",
            "Train | 69/100, BPR loss: 3.0142384275677614e-05\n",
            "Valid | 69/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7422957077839666\n",
            "---------------------------------------------------\n",
            "Train | 70/100, BPR loss: 2.9639022613991983e-05\n",
            "Valid | 70/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7424121012577318\n",
            "---------------------------------------------------\n",
            "Train | 71/100, BPR loss: 2.920174847531598e-05\n",
            "Valid | 71/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7424164213834897\n",
            "---------------------------------------------------\n",
            "Train | 72/100, BPR loss: 2.8769196433131583e-05\n",
            "Valid | 72/100, Pre@10: 0.16744432661717923, Rec@10: 0.07747411805112604, NDCG@10: 0.7424636347349858\n",
            "---------------------------------------------------\n",
            "Train | 73/100, BPR loss: 2.8303771614446305e-05\n",
            "Valid | 73/100, Pre@10: 0.1672322375397667, Rec@10: 0.07737598743928169, NDCG@10: 0.7425352315662637\n",
            "---------------------------------------------------\n",
            "Train | 74/100, BPR loss: 2.7886311727343127e-05\n",
            "Valid | 74/100, Pre@10: 0.16733828207847296, Rec@10: 0.07742505274520387, NDCG@10: 0.7426493134039258\n",
            "---------------------------------------------------\n",
            "Train | 75/100, BPR loss: 2.7486081307870336e-05\n",
            "Valid | 75/100, Pre@10: 0.16733828207847296, Rec@10: 0.07742505274520387, NDCG@10: 0.7424860112443626\n",
            "---------------------------------------------------\n",
            "Train | 76/100, BPR loss: 2.7154155759490095e-05\n",
            "Valid | 76/100, Pre@10: 0.16733828207847296, Rec@10: 0.07742505274520387, NDCG@10: 0.7423611284898057\n",
            "---------------------------------------------------\n",
            "Train | 77/100, BPR loss: 2.6778419851325452e-05\n",
            "Valid | 77/100, Pre@10: 0.16733828207847296, Rec@10: 0.07742505274520387, NDCG@10: 0.7417903817372703\n",
            "---------------------------------------------------\n",
            "Train | 78/100, BPR loss: 2.6366957172285765e-05\n",
            "Valid | 78/100, Pre@10: 0.16733828207847296, Rec@10: 0.07742505274520387, NDCG@10: 0.7423354718651215\n",
            "---------------------------------------------------\n",
            "Train | 79/100, BPR loss: 2.6055382477352396e-05\n",
            "Valid | 79/100, Pre@10: 0.16744432661717923, Rec@10: 0.07747411805112604, NDCG@10: 0.7423990674691016\n",
            "---------------------------------------------------\n",
            "Train | 80/100, BPR loss: 2.5741130230017006e-05\n",
            "Valid | 80/100, Pre@10: 0.16744432661717923, Rec@10: 0.07747411805112604, NDCG@10: 0.7426978610018381\n",
            "---------------------------------------------------\n",
            "Train | 81/100, BPR loss: 2.5402792743989266e-05\n",
            "Valid | 81/100, Pre@10: 0.16765641569459172, Rec@10: 0.07757224866297041, NDCG@10: 0.7425061911535661\n",
            "---------------------------------------------------\n",
            "Train | 82/100, BPR loss: 2.5056426238734275e-05\n",
            "Valid | 82/100, Pre@10: 0.16744432661717923, Rec@10: 0.07747411805112604, NDCG@10: 0.7426503429829032\n",
            "---------------------------------------------------\n",
            "Train | 83/100, BPR loss: 2.48153846769128e-05\n",
            "Valid | 83/100, Pre@10: 0.16744432661717923, Rec@10: 0.07747411805112604, NDCG@10: 0.7428697790176093\n",
            "---------------------------------------------------\n",
            "Train | 84/100, BPR loss: 2.4513226890121587e-05\n",
            "Valid | 84/100, Pre@10: 0.16755037115588547, Rec@10: 0.07752318335704823, NDCG@10: 0.7427851838569619\n",
            "---------------------------------------------------\n",
            "Train | 85/100, BPR loss: 2.420391138002742e-05\n",
            "Valid | 85/100, Pre@10: 0.167762460233298, Rec@10: 0.0776213139688926, NDCG@10: 0.7423821159285099\n",
            "---------------------------------------------------\n",
            "Train | 86/100, BPR loss: 2.3891938326414675e-05\n",
            "Valid | 86/100, Pre@10: 0.167762460233298, Rec@10: 0.0776213139688926, NDCG@10: 0.7424266588443288\n",
            "---------------------------------------------------\n",
            "Train | 87/100, BPR loss: 2.3647351554245688e-05\n",
            "Valid | 87/100, Pre@10: 0.167762460233298, Rec@10: 0.0776213139688926, NDCG@10: 0.7424475973862655\n",
            "---------------------------------------------------\n",
            "Train | 88/100, BPR loss: 2.337811019970104e-05\n",
            "Valid | 88/100, Pre@10: 0.167762460233298, Rec@10: 0.0776213139688926, NDCG@10: 0.7423499513976566\n",
            "---------------------------------------------------\n",
            "Train | 89/100, BPR loss: 2.3208378479466774e-05\n",
            "Valid | 89/100, Pre@10: 0.167762460233298, Rec@10: 0.0776213139688926, NDCG@10: 0.7421888293828048\n",
            "---------------------------------------------------\n",
            "Train | 90/100, BPR loss: 2.29271699936362e-05\n",
            "Valid | 90/100, Pre@10: 0.167762460233298, Rec@10: 0.0776213139688926, NDCG@10: 0.7425971637500975\n",
            "---------------------------------------------------\n",
            "Train | 91/100, BPR loss: 2.268529169668909e-05\n",
            "Valid | 91/100, Pre@10: 0.167762460233298, Rec@10: 0.0776213139688926, NDCG@10: 0.7425513848198964\n",
            "---------------------------------------------------\n",
            "Train | 92/100, BPR loss: 2.246262738481164e-05\n",
            "Valid | 92/100, Pre@10: 0.168186638388123, Rec@10: 0.07781757519258133, NDCG@10: 0.7425321732914465\n",
            "---------------------------------------------------\n",
            "Train | 93/100, BPR loss: 2.2251728296396323e-05\n",
            "Valid | 93/100, Pre@10: 0.16808059384941676, Rec@10: 0.07776850988665915, NDCG@10: 0.7425020534589335\n",
            "---------------------------------------------------\n",
            "Train | 94/100, BPR loss: 2.2026060833013617e-05\n",
            "Valid | 94/100, Pre@10: 0.16829268292682928, Rec@10: 0.0778666404985035, NDCG@10: 0.7424426402760269\n",
            "---------------------------------------------------\n",
            "Train | 95/100, BPR loss: 2.1866073439014144e-05\n",
            "Valid | 95/100, Pre@10: 0.16839872746553552, Rec@10: 0.0779157058044257, NDCG@10: 0.7423460950471968\n",
            "---------------------------------------------------\n",
            "Train | 96/100, BPR loss: 2.16632652154658e-05\n",
            "Valid | 96/100, Pre@10: 0.16839872746553552, Rec@10: 0.0779157058044257, NDCG@10: 0.7423052533156032\n",
            "---------------------------------------------------\n",
            "Train | 97/100, BPR loss: 2.1466112229973078e-05\n",
            "Valid | 97/100, Pre@10: 0.16829268292682928, Rec@10: 0.0778666404985035, NDCG@10: 0.7423714442929977\n",
            "---------------------------------------------------\n",
            "Train | 98/100, BPR loss: 2.1247418771963567e-05\n",
            "Valid | 98/100, Pre@10: 0.16839872746553552, Rec@10: 0.0779157058044257, NDCG@10: 0.7423893523587151\n",
            "---------------------------------------------------\n",
            "Train | 99/100, BPR loss: 2.103995757352095e-05\n",
            "Valid | 99/100, Pre@10: 0.16839872746553552, Rec@10: 0.0779157058044257, NDCG@10: 0.7423724851854006\n",
            "---------------------------------------------------\n",
            "Train | 100/100, BPR loss: 2.088639848807361e-05\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-19b7cc498b42>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, k, m, stepsize, max_iter, batch, n)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train | {x+1}/{max_iter}, BPR loss: {loss / batch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperformance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Valid | {x+1}/{max_iter}, Pre@{n}: {pre}, Rec@{n}: {rec}, NDCG@{n}: {ndcg}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---------------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-19b7cc498b42>\u001b[0m in \u001b[0;36mperformance\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mn_recall\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknown_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mn_precision\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mndcg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDCG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhit\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_recall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-19b7cc498b42>\u001b[0m in \u001b[0;36mNDCG\u001b[0;34m(self, uid, n)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# 取出模型排序下 merge 到的真实分数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mrating_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrating\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'serial_iid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pred_rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrating_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'serial_iid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mjoin_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         result = self._reindex_and_concat(\n\u001b[0m\u001b[1;32m    812\u001b[0m             \u001b[0mjoin_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_reindex_and_concat\u001b[0;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         llabels, rlabels = _items_overlap_with_suffix(\n\u001b[0m\u001b[1;32m    764\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_items_overlap_with_suffix\u001b[0;34m(left, right, suffixes)\u001b[0m\n\u001b[1;32m   2595\u001b[0m         )\n\u001b[1;32m   2596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m     \u001b[0mto_rename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2598\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_rename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mintersection\u001b[0;34m(self, other, sort)\u001b[0m\n\u001b[1;32m   3371\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3373\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3374\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_intersection_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_intersection\u001b[0;34m(self, other, sort)\u001b[0m\n\u001b[1;32m   3400\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mensure_wrapped_if_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3402\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intersection_via_get_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3403\u001b[0m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_try_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_intersection_via_get_indexer\u001b[0;34m(self, other, sort)\u001b[0m\n\u001b[1;32m   3421\u001b[0m         \"\"\"\n\u001b[1;32m   3422\u001b[0m         \u001b[0mleft_unique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3423\u001b[0;31m         \u001b[0mright_unique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3425\u001b[0m         \u001b[0;31m# even though we are unique, we need get_indexer_for for IntervalIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m   2908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2910\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2912\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_view\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_references\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_IndexT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_IndexT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m         \"\"\"\n\u001b[1;32m    762\u001b[0m         \u001b[0mfastpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmake\u001b[0m \u001b[0ma\u001b[0m \u001b[0mshallow\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msame\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model1.fit(k = 50, max_iter = 100, stepsize=1e-4, m=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88834487",
      "metadata": {
        "id": "88834487"
      },
      "source": [
        "### 1M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "49eca292",
      "metadata": {
        "id": "49eca292"
      },
      "outputs": [],
      "source": [
        "model2 = BPR()\n",
        "model2.preprocess(df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "d2969e2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2969e2c",
        "outputId": "7e0ef3a0-6223-4778-b647-1bc6bb4a9ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train | 1/100, BPR loss: 6.935479905223474e-05\n",
            "Valid | 1/100, Pre@10: 0.11827814569536424, Rec@10: 0.03528755106173839, NDCG@10: 0.7463039175082755\n",
            "---------------------------------------------------\n",
            "Train | 2/100, BPR loss: 7.562005339423195e-05\n",
            "Valid | 2/100, Pre@10: 0.10225165562913907, Rec@10: 0.030506147166474852, NDCG@10: 0.7241160138369586\n",
            "---------------------------------------------------\n",
            "Train | 3/100, BPR loss: 7.617966184625402e-05\n",
            "Valid | 3/100, Pre@10: 0.12377483443708609, Rec@10: 0.03692745405060978, NDCG@10: 0.7267446190120156\n",
            "---------------------------------------------------\n",
            "Train | 4/100, BPR loss: 7.621314580319449e-05\n",
            "Valid | 4/100, Pre@10: 0.1579635761589404, Rec@10: 0.04712745306271641, NDCG@10: 0.7500836802670748\n",
            "---------------------------------------------------\n",
            "Train | 5/100, BPR loss: 7.61510236770846e-05\n",
            "Valid | 5/100, Pre@10: 0.17009933774834438, Rec@10: 0.05074808225200172, NDCG@10: 0.7545036812478874\n",
            "---------------------------------------------------\n",
            "Train | 6/100, BPR loss: 7.598206138936803e-05\n",
            "Valid | 6/100, Pre@10: 0.17054635761589404, Rec@10: 0.050881447856518366, NDCG@10: 0.7562901422502136\n",
            "---------------------------------------------------\n",
            "Train | 7/100, BPR loss: 7.564690167782828e-05\n",
            "Valid | 7/100, Pre@10: 0.16932119205298013, Rec@10: 0.05051592731080607, NDCG@10: 0.7566245292402348\n",
            "---------------------------------------------------\n",
            "Train | 8/100, BPR loss: 7.510556315537542e-05\n",
            "Valid | 8/100, Pre@10: 0.16990066225165562, Rec@10: 0.05068880864999432, NDCG@10: 0.7566471740902415\n",
            "---------------------------------------------------\n",
            "Train | 9/100, BPR loss: 7.432475103996694e-05\n",
            "Valid | 9/100, Pre@10: 0.1709933774834437, Rec@10: 0.051014813461035015, NDCG@10: 0.756960260054244\n",
            "---------------------------------------------------\n",
            "Train | 10/100, BPR loss: 7.332862151088193e-05\n",
            "Valid | 10/100, Pre@10: 0.17178807947019867, Rec@10: 0.05125190786906461, NDCG@10: 0.7572375977702266\n",
            "---------------------------------------------------\n",
            "Train | 11/100, BPR loss: 7.208980241557583e-05\n",
            "Valid | 11/100, Pre@10: 0.17170529801324502, Rec@10: 0.05122721053489487, NDCG@10: 0.7569618805397109\n",
            "---------------------------------------------------\n",
            "Train | 12/100, BPR loss: 7.065702811814845e-05\n",
            "Valid | 12/100, Pre@10: 0.17390728476821193, Rec@10: 0.051884159623810204, NDCG@10: 0.7568328869809106\n",
            "---------------------------------------------------\n",
            "Train | 13/100, BPR loss: 6.903595931362361e-05\n",
            "Valid | 13/100, Pre@10: 0.17274834437086092, Rec@10: 0.05153839694543371, NDCG@10: 0.756879209984469\n",
            "---------------------------------------------------\n",
            "Train | 14/100, BPR loss: 6.73228278174065e-05\n",
            "Valid | 14/100, Pre@10: 0.17309602649006622, Rec@10: 0.05164212574894666, NDCG@10: 0.7569668309728095\n",
            "---------------------------------------------------\n",
            "Train | 15/100, BPR loss: 6.546267832163721e-05\n",
            "Valid | 15/100, Pre@10: 0.17259933774834438, Rec@10: 0.051493941743928164, NDCG@10: 0.7575602065036338\n",
            "---------------------------------------------------\n",
            "Train | 16/100, BPR loss: 6.348643364617601e-05\n",
            "Valid | 16/100, Pre@10: 0.1727814569536424, Rec@10: 0.05154827587910161, NDCG@10: 0.7573891392507587\n",
            "---------------------------------------------------\n",
            "Train | 17/100, BPR loss: 6.148018292151392e-05\n",
            "Valid | 17/100, Pre@10: 0.17153973509933776, Rec@10: 0.051177815866555364, NDCG@10: 0.7569626920750381\n",
            "---------------------------------------------------\n",
            "Train | 18/100, BPR loss: 5.9438025346025825e-05\n",
            "Valid | 18/100, Pre@10: 0.17109271523178807, Rec@10: 0.051044450262038715, NDCG@10: 0.7568374098815077\n",
            "---------------------------------------------------\n",
            "Train | 19/100, BPR loss: 5.739540938520804e-05\n",
            "Valid | 19/100, Pre@10: 0.17084437086092716, Rec@10: 0.05097035825952947, NDCG@10: 0.7570675225053777\n",
            "---------------------------------------------------\n",
            "Train | 20/100, BPR loss: 5.534593219636008e-05\n",
            "Valid | 20/100, Pre@10: 0.17102649006622517, Rec@10: 0.05102469239470291, NDCG@10: 0.7571869176524222\n",
            "---------------------------------------------------\n",
            "Train | 21/100, BPR loss: 5.3348921937868e-05\n",
            "Valid | 21/100, Pre@10: 0.17178807947019867, Rec@10: 0.05125190786906461, NDCG@10: 0.7570605503857916\n",
            "---------------------------------------------------\n",
            "Train | 22/100, BPR loss: 5.1392555178608745e-05\n",
            "Valid | 22/100, Pre@10: 0.17317880794701987, Rec@10: 0.05166682308311641, NDCG@10: 0.7569495831480141\n",
            "---------------------------------------------------\n",
            "Train | 23/100, BPR loss: 4.95501262776088e-05\n",
            "Valid | 23/100, Pre@10: 0.1728973509933775, Rec@10: 0.05158285214693926, NDCG@10: 0.7572002159414823\n",
            "---------------------------------------------------\n",
            "Train | 24/100, BPR loss: 4.770969826495275e-05\n",
            "Valid | 24/100, Pre@10: 0.17341059602649006, Rec@10: 0.05173597561879171, NDCG@10: 0.7573485639070706\n",
            "---------------------------------------------------\n",
            "Train | 25/100, BPR loss: 4.5994056563358754e-05\n",
            "Valid | 25/100, Pre@10: 0.1733774834437086, Rec@10: 0.05172609668512381, NDCG@10: 0.7572109296689602\n",
            "---------------------------------------------------\n",
            "Train | 26/100, BPR loss: 4.43897042714525e-05\n",
            "Valid | 26/100, Pre@10: 0.17276490066225167, Rec@10: 0.05154333641226766, NDCG@10: 0.7571630994484988\n",
            "---------------------------------------------------\n",
            "Train | 27/100, BPR loss: 4.286057082936168e-05\n",
            "Valid | 27/100, Pre@10: 0.17266556291390728, Rec@10: 0.05151369961126396, NDCG@10: 0.7571049481048002\n",
            "---------------------------------------------------\n",
            "Train | 28/100, BPR loss: 4.14181558880955e-05\n",
            "Valid | 28/100, Pre@10: 0.17256622516556291, Rec@10: 0.05148406281026026, NDCG@10: 0.75705512550932\n",
            "---------------------------------------------------\n",
            "Train | 29/100, BPR loss: 4.011049895780161e-05\n",
            "Valid | 29/100, Pre@10: 0.17240066225165562, Rec@10: 0.05143466814192076, NDCG@10: 0.7569708036851324\n",
            "---------------------------------------------------\n",
            "Train | 30/100, BPR loss: 3.8921472878428176e-05\n",
            "Valid | 30/100, Pre@10: 0.17248344370860927, Rec@10: 0.051459365476090514, NDCG@10: 0.756867103671819\n",
            "---------------------------------------------------\n",
            "Train | 31/100, BPR loss: 3.772525815293193e-05\n",
            "Valid | 31/100, Pre@10: 0.17274834437086092, Rec@10: 0.05153839694543371, NDCG@10: 0.7571600319257921\n",
            "---------------------------------------------------\n",
            "Train | 32/100, BPR loss: 3.670659498311579e-05\n",
            "Valid | 32/100, Pre@10: 0.17360927152317882, Rec@10: 0.05179524922079911, NDCG@10: 0.7567401422160965\n",
            "---------------------------------------------------\n",
            "Train | 33/100, BPR loss: 3.5723671317100525e-05\n",
            "Valid | 33/100, Pre@10: 0.173841059602649, Rec@10: 0.05186440175647441, NDCG@10: 0.7568998064206723\n",
            "---------------------------------------------------\n",
            "Train | 34/100, BPR loss: 3.4837557905120775e-05\n",
            "Valid | 34/100, Pre@10: 0.1742549668874172, Rec@10: 0.05198788842732315, NDCG@10: 0.7569396932138027\n",
            "---------------------------------------------------\n",
            "Train | 35/100, BPR loss: 3.3956297556869686e-05\n",
            "Valid | 35/100, Pre@10: 0.1731456953642384, Rec@10: 0.051656944149448505, NDCG@10: 0.7569848973507026\n",
            "---------------------------------------------------\n",
            "Train | 36/100, BPR loss: 3.337454472784884e-05\n",
            "Valid | 36/100, Pre@10: 0.17395695364238412, Rec@10: 0.05189897802431206, NDCG@10: 0.7573090812256412\n",
            "---------------------------------------------------\n",
            "Train | 37/100, BPR loss: 3.262388418079354e-05\n",
            "Valid | 37/100, Pre@10: 0.17412251655629138, Rec@10: 0.051948372692651554, NDCG@10: 0.7573862368670099\n",
            "---------------------------------------------------\n",
            "Train | 38/100, BPR loss: 3.203543019481003e-05\n",
            "Valid | 38/100, Pre@10: 0.17397350993377483, Rec@10: 0.05190391749114601, NDCG@10: 0.7572038175541248\n",
            "---------------------------------------------------\n",
            "Train | 39/100, BPR loss: 3.151418059132993e-05\n",
            "Valid | 39/100, Pre@10: 0.1735430463576159, Rec@10: 0.05177549135346331, NDCG@10: 0.7571701983278075\n",
            "---------------------------------------------------\n",
            "Train | 40/100, BPR loss: 3.1035520805744454e-05\n",
            "Valid | 40/100, Pre@10: 0.17367549668874172, Rec@10: 0.051815007088134905, NDCG@10: 0.7570409068728723\n",
            "---------------------------------------------------\n",
            "Train | 41/100, BPR loss: 3.0341016099555418e-05\n",
            "Valid | 41/100, Pre@10: 0.17428807947019867, Rec@10: 0.05199776736099106, NDCG@10: 0.7567039908610248\n",
            "---------------------------------------------------\n",
            "Train | 42/100, BPR loss: 2.9981507395859808e-05\n",
            "Valid | 42/100, Pre@10: 0.17403973509933776, Rec@10: 0.0519236753584818, NDCG@10: 0.7569321504262829\n",
            "---------------------------------------------------\n",
            "Train | 43/100, BPR loss: 2.9651422664755955e-05\n",
            "Valid | 43/100, Pre@10: 0.17379139072847682, Rec@10: 0.051849583355972555, NDCG@10: 0.7569709390099596\n",
            "---------------------------------------------------\n",
            "Train | 44/100, BPR loss: 2.9357026505749673e-05\n",
            "Valid | 44/100, Pre@10: 0.1737748344370861, Rec@10: 0.051844643889138606, NDCG@10: 0.7572128446733475\n",
            "---------------------------------------------------\n",
            "Train | 45/100, BPR loss: 2.899631545005832e-05\n",
            "Valid | 45/100, Pre@10: 0.17369205298013246, Rec@10: 0.051819946554968854, NDCG@10: 0.7571095656331376\n",
            "---------------------------------------------------\n",
            "Train | 46/100, BPR loss: 2.867983130272478e-05\n",
            "Valid | 46/100, Pre@10: 0.17367549668874172, Rec@10: 0.051815007088134905, NDCG@10: 0.7572333631304229\n",
            "---------------------------------------------------\n",
            "Train | 47/100, BPR loss: 2.8420081434887834e-05\n",
            "Valid | 47/100, Pre@10: 0.17346026490066224, Rec@10: 0.051750794019293556, NDCG@10: 0.7571833090291467\n",
            "---------------------------------------------------\n",
            "Train | 48/100, BPR loss: 2.8050279070157558e-05\n",
            "Valid | 48/100, Pre@10: 0.17364238410596025, Rec@10: 0.05180512815446701, NDCG@10: 0.7571720860033517\n",
            "---------------------------------------------------\n",
            "Train | 49/100, BPR loss: 2.7755504561355338e-05\n",
            "Valid | 49/100, Pre@10: 0.17367549668874172, Rec@10: 0.051815007088134905, NDCG@10: 0.7571028979760616\n",
            "---------------------------------------------------\n",
            "Train | 50/100, BPR loss: 2.7661593776429072e-05\n",
            "Valid | 50/100, Pre@10: 0.173841059602649, Rec@10: 0.05186440175647441, NDCG@10: 0.7573298407774687\n",
            "---------------------------------------------------\n",
            "Train | 51/100, BPR loss: 2.7361778847989626e-05\n",
            "Valid | 51/100, Pre@10: 0.1741887417218543, Rec@10: 0.051968130559987356, NDCG@10: 0.7574751658506439\n",
            "---------------------------------------------------\n",
            "Train | 52/100, BPR loss: 2.7271562430541962e-05\n",
            "Valid | 52/100, Pre@10: 0.17428807947019867, Rec@10: 0.05199776736099106, NDCG@10: 0.757394856041324\n",
            "---------------------------------------------------\n",
            "Train | 53/100, BPR loss: 2.7055906684836373e-05\n",
            "Valid | 53/100, Pre@10: 0.17385761589403972, Rec@10: 0.05186934122330836, NDCG@10: 0.7573878089999732\n",
            "---------------------------------------------------\n",
            "Train | 54/100, BPR loss: 2.6894336770055816e-05\n",
            "Valid | 54/100, Pre@10: 0.17370860927152318, Rec@10: 0.0518248860218028, NDCG@10: 0.7574449992874359\n",
            "---------------------------------------------------\n",
            "Train | 55/100, BPR loss: 2.6802983484230936e-05\n",
            "Valid | 55/100, Pre@10: 0.17370860927152318, Rec@10: 0.0518248860218028, NDCG@10: 0.7573325804987356\n",
            "---------------------------------------------------\n",
            "Train | 56/100, BPR loss: 2.6603724109008908e-05\n",
            "Valid | 56/100, Pre@10: 0.17432119205298013, Rec@10: 0.052007646294658955, NDCG@10: 0.7574929662719668\n",
            "---------------------------------------------------\n",
            "Train | 57/100, BPR loss: 2.6467709176358767e-05\n",
            "Valid | 57/100, Pre@10: 0.17466887417218543, Rec@10: 0.0521113750981719, NDCG@10: 0.7575442688255289\n",
            "---------------------------------------------------\n",
            "Train | 58/100, BPR loss: 2.642854451551102e-05\n",
            "Valid | 58/100, Pre@10: 0.17448675496688743, Rec@10: 0.05205704096299845, NDCG@10: 0.7574132007612977\n",
            "---------------------------------------------------\n",
            "Train | 59/100, BPR loss: 2.6329535103286617e-05\n",
            "Valid | 59/100, Pre@10: 0.1740066225165563, Rec@10: 0.051913796424813904, NDCG@10: 0.7574420846803063\n",
            "---------------------------------------------------\n",
            "Train | 60/100, BPR loss: 2.6308845917810686e-05\n",
            "Valid | 60/100, Pre@10: 0.17447019867549668, Rec@10: 0.0520521014961645, NDCG@10: 0.7574225570632189\n",
            "---------------------------------------------------\n",
            "Train | 61/100, BPR loss: 2.6284926207154058e-05\n",
            "Valid | 61/100, Pre@10: 0.1742384105960265, Rec@10: 0.0519829489604892, NDCG@10: 0.7575548623268586\n",
            "---------------------------------------------------\n",
            "Train | 62/100, BPR loss: 2.611002855701372e-05\n",
            "Valid | 62/100, Pre@10: 0.1745860927152318, Rec@10: 0.05208667776400215, NDCG@10: 0.7573684593558185\n",
            "---------------------------------------------------\n",
            "Train | 63/100, BPR loss: 2.6081766918650828e-05\n",
            "Valid | 63/100, Pre@10: 0.17438741721854303, Rec@10: 0.05202740416199476, NDCG@10: 0.7572886131746047\n",
            "---------------------------------------------------\n",
            "Train | 64/100, BPR loss: 2.5885658033075742e-05\n",
            "Valid | 64/100, Pre@10: 0.17422185430463577, Rec@10: 0.051978009493655254, NDCG@10: 0.7573070987375482\n",
            "---------------------------------------------------\n",
            "Train | 65/100, BPR loss: 2.5878425731207244e-05\n",
            "Valid | 65/100, Pre@10: 0.17471854304635762, Rec@10: 0.052126193498673756, NDCG@10: 0.7572966473995769\n",
            "---------------------------------------------------\n",
            "Train | 66/100, BPR loss: 2.5738858312251978e-05\n",
            "Valid | 66/100, Pre@10: 0.17466887417218543, Rec@10: 0.0521113750981719, NDCG@10: 0.7572487491417623\n",
            "---------------------------------------------------\n",
            "Train | 67/100, BPR loss: 2.5780122086871415e-05\n",
            "Valid | 67/100, Pre@10: 0.1745364238410596, Rec@10: 0.052071859363500304, NDCG@10: 0.7573431847247684\n",
            "---------------------------------------------------\n",
            "Train | 68/100, BPR loss: 2.5743791411514394e-05\n",
            "Valid | 68/100, Pre@10: 0.17513245033112582, Rec@10: 0.0522496801695225, NDCG@10: 0.7571732439669133\n",
            "---------------------------------------------------\n",
            "Train | 69/100, BPR loss: 2.574055724835489e-05\n",
            "Valid | 69/100, Pre@10: 0.1750496688741722, Rec@10: 0.052224982835352755, NDCG@10: 0.7570822190753341\n",
            "---------------------------------------------------\n",
            "Train | 70/100, BPR loss: 2.580400905571878e-05\n",
            "Valid | 70/100, Pre@10: 0.1753476821192053, Rec@10: 0.05231389323836385, NDCG@10: 0.7568684039478117\n",
            "---------------------------------------------------\n",
            "Train | 71/100, BPR loss: 2.5555109459673986e-05\n",
            "Valid | 71/100, Pre@10: 0.1754635761589404, Rec@10: 0.0523484695062015, NDCG@10: 0.7568589718417312\n",
            "---------------------------------------------------\n",
            "Train | 72/100, BPR loss: 2.5452065528952517e-05\n",
            "Valid | 72/100, Pre@10: 0.1752814569536424, Rec@10: 0.052294135371028054, NDCG@10: 0.7568860472971214\n",
            "---------------------------------------------------\n",
            "Train | 73/100, BPR loss: 2.5356941478094086e-05\n",
            "Valid | 73/100, Pre@10: 0.1752980132450331, Rec@10: 0.052299074837862, NDCG@10: 0.756914586195098\n",
            "---------------------------------------------------\n",
            "Train | 74/100, BPR loss: 2.5396602723049e-05\n",
            "Valid | 74/100, Pre@10: 0.1753476821192053, Rec@10: 0.05231389323836385, NDCG@10: 0.7568733083094612\n",
            "---------------------------------------------------\n",
            "Train | 75/100, BPR loss: 2.5241553885280155e-05\n",
            "Valid | 75/100, Pre@10: 0.17548013245033112, Rec@10: 0.05235340897303545, NDCG@10: 0.756854756166165\n",
            "---------------------------------------------------\n",
            "Train | 76/100, BPR loss: 2.531763311708346e-05\n",
            "Valid | 76/100, Pre@10: 0.17538079470198675, Rec@10: 0.05232377217203175, NDCG@10: 0.7571955238876472\n",
            "---------------------------------------------------\n",
            "Train | 77/100, BPR loss: 2.537648288125638e-05\n",
            "Valid | 77/100, Pre@10: 0.175, Rec@10: 0.0522101644348509, NDCG@10: 0.7573726019110775\n",
            "---------------------------------------------------\n",
            "Train | 78/100, BPR loss: 2.5150780857075006e-05\n",
            "Valid | 78/100, Pre@10: 0.1752980132450331, Rec@10: 0.052299074837862, NDCG@10: 0.7572358177186542\n",
            "---------------------------------------------------\n",
            "Train | 79/100, BPR loss: 2.5121202270383947e-05\n",
            "Valid | 79/100, Pre@10: 0.17536423841059604, Rec@10: 0.0523188327051978, NDCG@10: 0.7572990660291363\n",
            "---------------------------------------------------\n",
            "Train | 80/100, BPR loss: 2.5091796487686224e-05\n",
            "Valid | 80/100, Pre@10: 0.1753476821192053, Rec@10: 0.05231389323836385, NDCG@10: 0.7571219411523622\n",
            "---------------------------------------------------\n",
            "Train | 81/100, BPR loss: 2.5059422114281915e-05\n",
            "Valid | 81/100, Pre@10: 0.17521523178807946, Rec@10: 0.05227437750369225, NDCG@10: 0.7570584414299648\n",
            "---------------------------------------------------\n",
            "Train | 82/100, BPR loss: 2.535732346586883e-05\n",
            "Valid | 82/100, Pre@10: 0.17506622516556292, Rec@10: 0.052229922302186704, NDCG@10: 0.7571166368521121\n",
            "---------------------------------------------------\n",
            "Train | 83/100, BPR loss: 2.5059145627892576e-05\n",
            "Valid | 83/100, Pre@10: 0.17503311258278145, Rec@10: 0.0522200433685188, NDCG@10: 0.7570716382992007\n",
            "---------------------------------------------------\n",
            "Train | 84/100, BPR loss: 2.4940280127339065e-05\n",
            "Valid | 84/100, Pre@10: 0.1752814569536424, Rec@10: 0.052294135371028054, NDCG@10: 0.7571633497248484\n",
            "---------------------------------------------------\n",
            "Train | 85/100, BPR loss: 2.511157981643919e-05\n",
            "Valid | 85/100, Pre@10: 0.17541390728476822, Rec@10: 0.05233365110569965, NDCG@10: 0.7571644145865675\n",
            "---------------------------------------------------\n",
            "Train | 86/100, BPR loss: 2.4976265194709413e-05\n",
            "Valid | 86/100, Pre@10: 0.17526490066225164, Rec@10: 0.0522891959041941, NDCG@10: 0.7571662028260077\n",
            "---------------------------------------------------\n",
            "Train | 87/100, BPR loss: 2.5063469365704805e-05\n",
            "Valid | 87/100, Pre@10: 0.1752814569536424, Rec@10: 0.052294135371028054, NDCG@10: 0.7571640463474375\n",
            "---------------------------------------------------\n",
            "Train | 88/100, BPR loss: 2.5011291654664092e-05\n",
            "Valid | 88/100, Pre@10: 0.17536423841059604, Rec@10: 0.0523188327051978, NDCG@10: 0.7571614194616062\n",
            "---------------------------------------------------\n",
            "Train | 89/100, BPR loss: 2.5095223463722505e-05\n",
            "Valid | 89/100, Pre@10: 0.17519867549668874, Rec@10: 0.0522694380368583, NDCG@10: 0.7573452740468749\n",
            "---------------------------------------------------\n",
            "Train | 90/100, BPR loss: 2.489738108124584e-05\n",
            "Valid | 90/100, Pre@10: 0.1754635761589404, Rec@10: 0.0523484695062015, NDCG@10: 0.7572180309846126\n",
            "---------------------------------------------------\n",
            "Train | 91/100, BPR loss: 2.503383620933164e-05\n",
            "Valid | 91/100, Pre@10: 0.17549668874172186, Rec@10: 0.052358348439869404, NDCG@10: 0.757190996066054\n",
            "---------------------------------------------------\n",
            "Train | 92/100, BPR loss: 2.4952161766123027e-05\n",
            "Valid | 92/100, Pre@10: 0.17503311258278145, Rec@10: 0.0522200433685188, NDCG@10: 0.7573845950562232\n",
            "---------------------------------------------------\n",
            "Train | 93/100, BPR loss: 2.4822291379678063e-05\n",
            "Valid | 93/100, Pre@10: 0.17526490066225164, Rec@10: 0.0522891959041941, NDCG@10: 0.7573591096479589\n",
            "---------------------------------------------------\n",
            "Train | 94/100, BPR loss: 2.491309351171367e-05\n",
            "Valid | 94/100, Pre@10: 0.17483443708609273, Rec@10: 0.052160769766511406, NDCG@10: 0.7574653123980207\n",
            "---------------------------------------------------\n",
            "Train | 95/100, BPR loss: 2.4957034838735126e-05\n",
            "Valid | 95/100, Pre@10: 0.1748841059602649, Rec@10: 0.05217558816701325, NDCG@10: 0.7571975342304994\n",
            "---------------------------------------------------\n",
            "Train | 96/100, BPR loss: 2.493449028406758e-05\n",
            "Valid | 96/100, Pre@10: 0.17516556291390728, Rec@10: 0.052259559103190405, NDCG@10: 0.7572149444364135\n",
            "---------------------------------------------------\n",
            "Train | 97/100, BPR loss: 2.4642960852361284e-05\n",
            "Valid | 97/100, Pre@10: 0.17526490066225164, Rec@10: 0.0522891959041941, NDCG@10: 0.7571221286531635\n",
            "---------------------------------------------------\n",
            "Train | 98/100, BPR loss: 2.47820917138597e-05\n",
            "Valid | 98/100, Pre@10: 0.17531456953642385, Rec@10: 0.05230401430469595, NDCG@10: 0.7570298828350148\n",
            "---------------------------------------------------\n",
            "Train | 99/100, BPR loss: 2.4920105715864338e-05\n",
            "Valid | 99/100, Pre@10: 0.17491721854304637, Rec@10: 0.05218546710068115, NDCG@10: 0.7571528601015568\n",
            "---------------------------------------------------\n",
            "Train | 100/100, BPR loss: 2.4822133127599955e-05\n",
            "Valid | 100/100, Pre@10: 0.17518211920529803, Rec@10: 0.052264498570024354, NDCG@10: 0.7572264398816091\n",
            "---------------------------------------------------\n",
            "CPU times: user 47min 44s, sys: 9.59 s, total: 47min 54s\n",
            "Wall time: 48min 9s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model2.fit(k = 50, max_iter = 100, stepsize=5e-4, m=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "370fa4da",
      "metadata": {
        "id": "370fa4da"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}